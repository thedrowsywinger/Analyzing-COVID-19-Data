{
  
    
        "post0": {
            "title": "Title",
            "content": "This dataset has been taken from: https://github.com/CSSEGISandData/COVID-19 . import pandas as pd import os from collections import Counter from tqdm import tqdm %matplotlib inline import matplotlib.pyplot as plt import seaborn as sns . cd /media/thedrowsywinger/2A24A59224A56195/Poralekha/github/COVID-19/csse_covid_19_data/csse_covid_19_daily_reports . /media/thedrowsywinger/2A24A59224A56195/Poralekha/github/COVID-19/csse_covid_19_data/csse_covid_19_daily_reports . ls . 01-22-2020.csv* 02-07-2020.csv* 02-23-2020.csv* 03-10-2020.csv* 01-23-2020.csv* 02-08-2020.csv* 02-24-2020.csv* 03-11-2020.csv* 01-24-2020.csv* 02-09-2020.csv* 02-25-2020.csv* 03-12-2020.csv* 01-25-2020.csv* 02-10-2020.csv* 02-26-2020.csv* 03-13-2020.csv* 01-26-2020.csv* 02-11-2020.csv* 02-27-2020.csv* 03-14-2020.csv* 01-27-2020.csv* 02-12-2020.csv* 02-28-2020.csv* 03-15-2020.csv* 01-28-2020.csv* 02-13-2020.csv* 02-29-2020.csv* 03-16-2020.csv* 01-29-2020.csv* 02-14-2020.csv* 03-01-2020.csv* 03-17-2020.csv* 01-30-2020.csv* 02-15-2020.csv* 03-02-2020.csv* 03-18-2020.csv* 01-31-2020.csv* 02-16-2020.csv* 03-03-2020.csv* 03-19-2020.csv* 02-01-2020.csv* 02-17-2020.csv* 03-04-2020.csv* 03-20-2020.csv* 02-02-2020.csv* 02-18-2020.csv* 03-05-2020.csv* 03-21-2020.csv* 02-03-2020.csv* 02-19-2020.csv* 03-06-2020.csv* 03-22-2020.csv* 02-04-2020.csv* 02-20-2020.csv* 03-07-2020.csv* 03-23-2020.csv* 02-05-2020.csv* 02-21-2020.csv* 03-08-2020.csv* README.md* 02-06-2020.csv* 02-22-2020.csv* 03-09-2020.csv* . What each CSV file looks like: . old_sample_file = pd.read_csv(&quot;./01-22-2020.csv&quot;) old_sample_file.head() . Province/State Country/Region Last Update Confirmed Deaths Recovered . 0 Anhui | Mainland China | 1/22/2020 17:00 | 1.0 | NaN | NaN | . 1 Beijing | Mainland China | 1/22/2020 17:00 | 14.0 | NaN | NaN | . 2 Chongqing | Mainland China | 1/22/2020 17:00 | 6.0 | NaN | NaN | . 3 Fujian | Mainland China | 1/22/2020 17:00 | 1.0 | NaN | NaN | . 4 Gansu | Mainland China | 1/22/2020 17:00 | NaN | NaN | NaN | . sample_file = pd.read_csv(&quot;./01-23-2020.csv&quot;) . confi = list(sample_file[&#39;Confirmed&#39;]) . all(isinstance(n, float) for n in confi) . True . import math . len(another) . 46 . Combining all of the csv files . list_of_all_files = os.listdir() actual_list = [] for i in list_of_all_files: if &quot;csv&quot; in i: actual_list.append(i) province_list = [] country_list = [] confirmed = [] death_count = [] date_list = [] t = len(actual_list) for j in tqdm(actual_list[0:t-1]): a = pd.read_csv(j) for i in range(len(a)): date_list.append(&quot;Date: &quot; + str(j[:-4])) for i in a[&#39;Province/State&#39;]: province_list.append(i) for i in a[&#39;Country/Region&#39;]: country_list.append(i) for i in a[&#39;Confirmed&#39;]: confirmed.append(i) for i in a[&#39;Deaths&#39;]: death_count.append(i) g = actual_list[t-1] n = pd.read_csv(g) for i in range(len(n)): date_list.append(&quot;Date: &quot; + str(actual_list[t-1])[:-4]) for i in n[&#39;Province_State&#39;]: province_list.append(i) for i in n[&#39;Country_Region&#39;]: country_list.append(i) for i in n[&#39;Confirmed&#39;]: confirmed.append(i) for i in n[&#39;Deaths&#39;]: death_count.append(i) . 100%|██████████| 61/61 [00:00&lt;00:00, 283.59it/s] . Checking the lengths of all the new lists created . all(isinstance(n, int) for n in confirmed[1:10]) . True . confirmed_cases = [] for i in confirmed: if math.isnan(i): a = 0 confirmed_cases.append(a) else: confirmed_cases.append(i) . print(len(province_list)) print(len(country_list)) print(len(confirmed_cases)) print(len(death_count)) print(len(date_list)) . 11341 11341 11341 11341 11341 . Alright looks like we are tracking everything perfectly . Let&#39;s create the main dataframe . main_df = pd.DataFrame() main_df[&#39;Date&#39;] = date_list main_df[&#39;Country&#39;] = country_list main_df[&#39;State&#39;] = province_list main_df[&quot;Confirmed&quot;] = confirmed_cases main_df[&#39;Deaths&#39;] = death_count . To check that our data is still accurate: let&#39;s track the csv file of this date . checker = pd.read_csv(&quot;./02-07-2020.csv&quot;) checker.head() . Province/State Country/Region Last Update Confirmed Deaths Recovered . 0 Hubei | Mainland China | 2020-02-07T23:43:02 | 24953 | 699 | 1115 | . 1 Guangdong | Mainland China | 2020-02-07T10:13:06 | 1034 | 1 | 88 | . 2 Zhejiang | Mainland China | 2020-02-07T11:33:11 | 1006 | 0 | 123 | . 3 Henan | Mainland China | 2020-02-07T14:03:12 | 914 | 3 | 86 | . 4 Hunan | Mainland China | 2020-02-07T11:33:11 | 772 | 0 | 112 | . Looks good so far . Top 10 countries in the records . top_ten = [] for i in list(Counter(country_list).most_common(10)): print(i[0]) top_ten.append(i[0]) . US Mainland China China Australia Canada France Japan Thailand United Kingdom Singapore . Creating a sample dataFrame to work with . sample_df = main_df[200:350] sample_df . Date Country State Confirmed Deaths . 200 Date: 01-22-2020 | Mainland China | Zhejiang | 10.0 | NaN | . 201 Date: 01-22-2020 | Japan | NaN | 2.0 | NaN | . 202 Date: 01-22-2020 | Thailand | NaN | 2.0 | NaN | . 203 Date: 01-22-2020 | South Korea | NaN | 1.0 | NaN | . 204 Date: 01-23-2020 | Mainland China | Anhui | 9.0 | NaN | . 205 Date: 01-23-2020 | Mainland China | Beijing | 22.0 | NaN | . 206 Date: 01-23-2020 | Mainland China | Chongqing | 9.0 | NaN | . 207 Date: 01-23-2020 | Mainland China | Fujian | 5.0 | NaN | . 208 Date: 01-23-2020 | Mainland China | Gansu | 2.0 | NaN | . 209 Date: 01-23-2020 | Mainland China | Guangdong | 32.0 | NaN | . 210 Date: 01-23-2020 | Mainland China | Guangxi | 5.0 | NaN | . 211 Date: 01-23-2020 | Mainland China | Guizhou | 3.0 | NaN | . 212 Date: 01-23-2020 | Mainland China | Hainan | 5.0 | NaN | . 213 Date: 01-23-2020 | Mainland China | Hebei | 1.0 | 1.0 | . 214 Date: 01-23-2020 | Mainland China | Heilongjiang | 2.0 | NaN | . 215 Date: 01-23-2020 | Mainland China | Henan | 5.0 | NaN | . 216 Date: 01-23-2020 | Hong Kong | Hong Kong | 2.0 | NaN | . 217 Date: 01-23-2020 | Mainland China | Hubei | 444.0 | 17.0 | . 218 Date: 01-23-2020 | Mainland China | Hunan | 9.0 | NaN | . 219 Date: 01-23-2020 | Mainland China | Inner Mongolia | 0.0 | NaN | . 220 Date: 01-23-2020 | Mainland China | Jiangsu | 5.0 | NaN | . 221 Date: 01-23-2020 | Mainland China | Jiangxi | 7.0 | NaN | . 222 Date: 01-23-2020 | Mainland China | Jilin | 1.0 | NaN | . 223 Date: 01-23-2020 | Mainland China | Liaoning | 3.0 | NaN | . 224 Date: 01-23-2020 | Macau | Macau | 2.0 | NaN | . 225 Date: 01-23-2020 | Mainland China | Ningxia | 1.0 | NaN | . 226 Date: 01-23-2020 | Mainland China | Qinghai | 0.0 | NaN | . 227 Date: 01-23-2020 | Mainland China | Shaanxi | 3.0 | NaN | . 228 Date: 01-23-2020 | Mainland China | Shandong | 6.0 | NaN | . 229 Date: 01-23-2020 | Mainland China | Shanghai | 16.0 | NaN | . ... ... | ... | ... | ... | ... | . 320 Date: 01-25-2020 | Taiwan | Taiwan | 3.0 | NaN | . 321 Date: 01-25-2020 | Mainland China | Xinjiang | 3.0 | NaN | . 322 Date: 01-25-2020 | Macau | Macau | 2.0 | NaN | . 323 Date: 01-25-2020 | Mainland China | Qinghai | 1.0 | NaN | . 324 Date: 01-25-2020 | US | Washington | 1.0 | NaN | . 325 Date: 01-25-2020 | US | Illinois | 1.0 | NaN | . 326 Date: 01-25-2020 | Japan | NaN | 2.0 | NaN | . 327 Date: 01-25-2020 | Thailand | NaN | 7.0 | NaN | . 328 Date: 01-25-2020 | South Korea | NaN | 2.0 | NaN | . 329 Date: 01-25-2020 | Singapore | NaN | 3.0 | NaN | . 330 Date: 01-25-2020 | Vietnam | NaN | 2.0 | NaN | . 331 Date: 01-25-2020 | France | NaN | 3.0 | NaN | . 332 Date: 01-25-2020 | Australia | NaN | 4.0 | NaN | . 333 Date: 01-25-2020 | Nepal | NaN | 1.0 | NaN | . 334 Date: 01-25-2020 | Malaysia | NaN | 3.0 | NaN | . 335 Date: 01-26-2020 | Mainland China | Hubei | 1058.0 | 52.0 | . 336 Date: 01-26-2020 | Mainland China | Guangdong | 111.0 | NaN | . 337 Date: 01-26-2020 | Mainland China | Zhejiang | 104.0 | NaN | . 338 Date: 01-26-2020 | Mainland China | Henan | 83.0 | 1.0 | . 339 Date: 01-26-2020 | Mainland China | Chongqing | 75.0 | NaN | . 340 Date: 01-26-2020 | Mainland China | Hunan | 69.0 | NaN | . 341 Date: 01-26-2020 | Mainland China | Beijing | 68.0 | NaN | . 342 Date: 01-26-2020 | Mainland China | Anhui | 60.0 | NaN | . 343 Date: 01-26-2020 | Mainland China | Shandong | 46.0 | NaN | . 344 Date: 01-26-2020 | Mainland China | Sichuan | 44.0 | NaN | . 345 Date: 01-26-2020 | Mainland China | Shanghai | 40.0 | 1.0 | . 346 Date: 01-26-2020 | Mainland China | Guangxi | 36.0 | NaN | . 347 Date: 01-26-2020 | Mainland China | Jiangxi | 36.0 | NaN | . 348 Date: 01-26-2020 | Mainland China | Fujian | 35.0 | NaN | . 349 Date: 01-26-2020 | Mainland China | Jiangsu | 33.0 | NaN | . 150 rows × 5 columns . duplicate_dates = date_list . all_dates = [] for x in duplicate_dates: if x not in all_dates: all_dates.append(x) . len(all_dates) . 62 . testing a code , that i&#39;ll later convert into a function, with the sample dataframe . all_date_dictionary = {} for i in all_dates: # print(&quot;Searching for:&quot;, i) country_list = [] confirmed_case = [] for column, row in sample_df.iterrows(): if i == row[&quot;Date&quot;]: if row[&quot;Country&quot;] in top_ten: if len(country_list) == 0: country_list.append(row[&quot;Country&quot;]) confirmed_case.append(row[&quot;Confirmed&quot;]) else: if row[&quot;Country&quot;] in country_list: search = country_list.index(row[&quot;Country&quot;]) confirmed_case[search] += row[&quot;Confirmed&quot;] else: country_list.append(row[&quot;Country&quot;]) confirmed_case.append(row[&quot;Confirmed&quot;]) temp_dictionary = {} for j in range(len(country_list)): temp_dictionary[country_list[j]] = confirmed_case[j] all_date_dictionary[i] = temp_dictionary print(all_date_dictionary) . {&#39;Date: 02-07-2020&#39;: {}, &#39;Date: 02-25-2020&#39;: {}, &#39;Date: 01-22-2020&#39;: {&#39;Mainland China&#39;: 10.0, &#39;Japan&#39;: 2.0, &#39;Thailand&#39;: 2.0}, &#39;Date: 01-23-2020&#39;: {&#39;Mainland China&#39;: 639.0, &#39;US&#39;: 1.0, &#39;Japan&#39;: 1.0, &#39;Thailand&#39;: 3.0, &#39;Singapore&#39;: 1.0, &#39;Australia&#39;: 0.0}, &#39;Date: 01-24-2020&#39;: {&#39;Mainland China&#39;: 916.0, &#39;US&#39;: 2.0, &#39;Japan&#39;: 2.0, &#39;Thailand&#39;: 5.0, &#39;Singapore&#39;: 3.0, &#39;France&#39;: 2.0}, &#39;Date: 01-25-2020&#39;: {&#39;Mainland China&#39;: 1399.0, &#39;US&#39;: 2.0, &#39;Japan&#39;: 2.0, &#39;Thailand&#39;: 7.0, &#39;Singapore&#39;: 3.0, &#39;France&#39;: 3.0, &#39;Australia&#39;: 4.0}, &#39;Date: 01-26-2020&#39;: {&#39;Mainland China&#39;: 1898.0}, &#39;Date: 01-27-2020&#39;: {}, &#39;Date: 01-28-2020&#39;: {}, &#39;Date: 01-29-2020&#39;: {}, &#39;Date: 01-30-2020&#39;: {}, &#39;Date: 01-31-2020&#39;: {}, &#39;Date: 02-01-2020&#39;: {}, &#39;Date: 02-02-2020&#39;: {}, &#39;Date: 02-03-2020&#39;: {}, &#39;Date: 02-04-2020&#39;: {}, &#39;Date: 02-05-2020&#39;: {}, &#39;Date: 02-06-2020&#39;: {}, &#39;Date: 02-08-2020&#39;: {}, &#39;Date: 02-09-2020&#39;: {}, &#39;Date: 02-10-2020&#39;: {}, &#39;Date: 02-11-2020&#39;: {}, &#39;Date: 02-12-2020&#39;: {}, &#39;Date: 02-13-2020&#39;: {}, &#39;Date: 02-14-2020&#39;: {}, &#39;Date: 02-15-2020&#39;: {}, &#39;Date: 02-16-2020&#39;: {}, &#39;Date: 02-17-2020&#39;: {}, &#39;Date: 02-18-2020&#39;: {}, &#39;Date: 02-19-2020&#39;: {}, &#39;Date: 02-20-2020&#39;: {}, &#39;Date: 02-21-2020&#39;: {}, &#39;Date: 02-22-2020&#39;: {}, &#39;Date: 02-23-2020&#39;: {}, &#39;Date: 02-24-2020&#39;: {}, &#39;Date: 02-26-2020&#39;: {}, &#39;Date: 02-27-2020&#39;: {}, &#39;Date: 02-28-2020&#39;: {}, &#39;Date: 02-29-2020&#39;: {}, &#39;Date: 03-01-2020&#39;: {}, &#39;Date: 03-02-2020&#39;: {}, &#39;Date: 03-03-2020&#39;: {}, &#39;Date: 03-04-2020&#39;: {}, &#39;Date: 03-05-2020&#39;: {}, &#39;Date: 03-06-2020&#39;: {}, &#39;Date: 03-07-2020&#39;: {}, &#39;Date: 03-08-2020&#39;: {}, &#39;Date: 03-09-2020&#39;: {}, &#39;Date: 03-10-2020&#39;: {}, &#39;Date: 03-11-2020&#39;: {}, &#39;Date: 03-12-2020&#39;: {}, &#39;Date: 03-13-2020&#39;: {}, &#39;Date: 03-14-2020&#39;: {}, &#39;Date: 03-15-2020&#39;: {}, &#39;Date: 03-16-2020&#39;: {}, &#39;Date: 03-17-2020&#39;: {}, &#39;Date: 03-18-2020&#39;: {}, &#39;Date: 03-19-2020&#39;: {}, &#39;Date: 03-20-2020&#39;: {}, &#39;Date: 03-21-2020&#39;: {}, &#39;Date: 03-22-2020&#39;: {}, &#39;Date: 03-23-2020&#39;: {}} . def date_dictionary(sample_df, all_dates, top_ten): all_date_dictionary = {} for i in all_dates: country_list = top_ten confirmed_case = [0] * len(top_ten) for column, row in sample_df.iterrows(): if i == row[&quot;Date&quot;]: if row[&quot;Country&quot;] in top_ten: if row[&quot;Country&quot;] in country_list: search = country_list.index(row[&quot;Country&quot;]) confirmed_case[search] += row[&quot;Confirmed&quot;] else: pass temp_dictionary = {} for j in range(len(country_list)): temp_dictionary[country_list[j]] = confirmed_case[j] all_date_dictionary[i] = temp_dictionary return all_date_dictionary . a = date_dictionary(main_df, all_dates, top_ten) . So the dictionary works, time to create a dataframe . trend_df = pd.DataFrame() . d_list = [] for key, value in a.items(): d_list.append(key) . len(d_list) . 62 . I need to get the case per date for all 62 dates in all the top countries . a[&#39;Date: 02-23-2020&#39;] # this shows i have case data on each date of each country . {&#39;US&#39;: 35.0, &#39;Mainland China&#39;: 76938.0, &#39;China&#39;: 0, &#39;Australia&#39;: 22.0, &#39;Canada&#39;: 9.0, &#39;France&#39;: 12.0, &#39;Japan&#39;: 147.0, &#39;Thailand&#39;: 35.0, &#39;United Kingdom&#39;: 0, &#39;Singapore&#39;: 89.0} . So i guess i could have done this in an easier way, but .... oh well . country_dictionary = {} for i in top_ten: case_per_date = [] for key,value in a.items(): for k,v in value.items(): if i == k: case_per_date.append(v) country_dictionary[i] = case_per_date . I just created a dictionary, where each key is the name of a country, and the value is a 62 length list containing the number of cases on each day . Alright, back to the data frame . trend_df[&quot;Date&quot;] = d_list for key, value in country_dictionary.items(): trend_df[key] = value . trend_df.head() . Date US Mainland China China Australia Canada France Japan Thailand United Kingdom Singapore . 0 Date: 02-07-2020 | 12.0 | 34075.0 | 0.0 | 15.0 | 7.0 | 6.0 | 25.0 | 25.0 | 0.0 | 30.0 | . 1 Date: 02-25-2020 | 53.0 | 77660.0 | 0.0 | 22.0 | 11.0 | 14.0 | 170.0 | 37.0 | 0.0 | 91.0 | . 2 Date: 01-22-2020 | 1.0 | 547.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 2.0 | 0.0 | 0.0 | . 3 Date: 01-23-2020 | 1.0 | 639.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 3.0 | 0.0 | 1.0 | . 4 Date: 01-24-2020 | 2.0 | 916.0 | 0.0 | 0.0 | 0.0 | 2.0 | 2.0 | 5.0 | 0.0 | 3.0 | . Phew! Alhamdulillah . I can finally go to plotting . ax = plt.figure(figsize=(20,10)) # Add title ax = plt.title(&quot;Trend of COVID-19 confirmed cases in the most affected countries&quot;) for i in top_ten: ax = sns.lineplot(y = trend_df[i], x = trend_df[&#39;Date&#39;], label = i) ax.legend() # ax = sns.color_palette(&quot;RdBu&quot;, n_colors=7) ax.set_xticklabels(labels=trend_df[&#39;Date&#39;], rotation=45, ha=&#39;right&#39;) ax = plt.ylabel(&quot;Count&quot;) . Gosh that spike of Mainland China, Let&#39;s verify . old_sample_file = pd.read_csv(&quot;./02-19-2020.csv&quot;) old_sample_file.head() . Province/State Country/Region Last Update Confirmed Deaths Recovered . 0 Hubei | Mainland China | 2020-02-19T23:23:02 | 62031 | 2029 | 10337 | . 1 Guangdong | Mainland China | 2020-02-19T10:23:02 | 1331 | 5 | 606 | . 2 Henan | Mainland China | 2020-02-19T12:13:08 | 1262 | 19 | 573 | . 3 Zhejiang | Mainland China | 2020-02-19T11:33:02 | 1174 | 0 | 604 | . 4 Hunan | Mainland China | 2020-02-19T11:33:02 | 1008 | 4 | 561 | . Whoa . I understand this graph is really unintuitive, will try to make it more user friendly . I also created this other dictionary, it was a dead end for this task, but might be helpful later . def dictionary_output(sample_df, top_ten): new_country_list = [] new_state_list = [] new_confirmed_list = [] new_death_list = [] new_date_list = [] top_country_dictionary = {} for i in top_ten: counter = 0 date_list = [] temp = [] print(&quot;Searching for: &quot;, i) for column , row in sample_df.iterrows(): if i == row[&quot;Country&quot;]: # print(&quot;Found a match!&quot;) # print(&quot;Current&quot;, row[&#39;Date&#39;], &quot;Confirmed cases: &quot;, row[&#39;Confirmed&#39;]) if len(date_list) == 0: date_list.append(row[&quot;Date&quot;]) temp.append(row[&#39;Confirmed&#39;]) else: if date_list[-1] == row[&quot;Date&quot;]: if type(row[&quot;Confirmed&quot;]) == type(float(&#39;nan&#39;)): temp[-1] += 0 else: temp[-1]+=row[&quot;Confirmed&quot;] else: date_list.append(row[&quot;Date&quot;]) temp.append(row[&#39;Confirmed&#39;]) date_dictionary = {} for j in range(len(date_list)): date_dictionary[date_list[j]] = temp[j] top_country_dictionary[i] = date_dictionary return top_country_dictionary . a = dictionary_output(main_df, top_ten) . Searching for: US Searching for: Mainland China Searching for: China Searching for: Australia Searching for: Canada Searching for: France Searching for: Japan Searching for: Thailand Searching for: United Kingdom Searching for: Singapore .",
            "url": "https://thedrowsywinger.github.io/Analyzing-COVID-19-Data/2020/02/23/Trends-in-Top-Countries.html",
            "relUrl": "/2020/02/23/Trends-in-Top-Countries.html",
            "date": " • Feb 23, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc: true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://thedrowsywinger.github.io/Analyzing-COVID-19-Data/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Title",
            "content": "This dataset has been taken from: https://github.com/CSSEGISandData/COVID-19 . import pandas as pd import os from collections import Counter from tqdm import tqdm %matplotlib inline import matplotlib.pyplot as plt import seaborn as sns . cd /media/thedrowsywinger/2A24A59224A56195/Poralekha/github/COVID-19/csse_covid_19_data/csse_covid_19_daily_reports . /media/thedrowsywinger/2A24A59224A56195/Poralekha/github/COVID-19/csse_covid_19_data/csse_covid_19_daily_reports . ls . 01-22-2020.csv* 02-07-2020.csv* 02-23-2020.csv* 03-10-2020.csv* 01-23-2020.csv* 02-08-2020.csv* 02-24-2020.csv* 03-11-2020.csv* 01-24-2020.csv* 02-09-2020.csv* 02-25-2020.csv* 03-12-2020.csv* 01-25-2020.csv* 02-10-2020.csv* 02-26-2020.csv* 03-13-2020.csv* 01-26-2020.csv* 02-11-2020.csv* 02-27-2020.csv* 03-14-2020.csv* 01-27-2020.csv* 02-12-2020.csv* 02-28-2020.csv* 03-15-2020.csv* 01-28-2020.csv* 02-13-2020.csv* 02-29-2020.csv* 03-16-2020.csv* 01-29-2020.csv* 02-14-2020.csv* 03-01-2020.csv* 03-17-2020.csv* 01-30-2020.csv* 02-15-2020.csv* 03-02-2020.csv* 03-18-2020.csv* 01-31-2020.csv* 02-16-2020.csv* 03-03-2020.csv* 03-19-2020.csv* 02-01-2020.csv* 02-17-2020.csv* 03-04-2020.csv* 03-20-2020.csv* 02-02-2020.csv* 02-18-2020.csv* 03-05-2020.csv* 03-21-2020.csv* 02-03-2020.csv* 02-19-2020.csv* 03-06-2020.csv* 03-22-2020.csv* 02-04-2020.csv* 02-20-2020.csv* 03-07-2020.csv* README.md* 02-05-2020.csv* 02-21-2020.csv* 03-08-2020.csv* 02-06-2020.csv* 02-22-2020.csv* 03-09-2020.csv* . a = pd.read_csv(&quot;/media/thedrowsywinger/2A24A59224A56195/Poralekha/github/COVID-19/csse_covid_19_data/csse_covid_19_daily_reports/01-23-2020.csv&quot;) . What each CSV file looks like . a.head() . Province/State Country/Region Last Update Confirmed Deaths Recovered . 0 Anhui | Mainland China | 1/23/20 17:00 | 9.0 | NaN | NaN | . 1 Beijing | Mainland China | 1/23/20 17:00 | 22.0 | NaN | NaN | . 2 Chongqing | Mainland China | 1/23/20 17:00 | 9.0 | NaN | NaN | . 3 Fujian | Mainland China | 1/23/20 17:00 | 5.0 | NaN | NaN | . 4 Gansu | Mainland China | 1/23/20 17:00 | 2.0 | NaN | NaN | . province_list = [] for i in a[&#39;Province/State&#39;]: province_list.append(i) . len(province_list) . 46 . province_counts = Counter(province_list) . province_counts . Counter({&#39;Anhui&#39;: 1, &#39;Beijing&#39;: 1, &#39;Chongqing&#39;: 1, &#39;Fujian&#39;: 1, &#39;Gansu&#39;: 1, &#39;Guangdong&#39;: 1, &#39;Guangxi&#39;: 1, &#39;Guizhou&#39;: 1, &#39;Hainan&#39;: 1, &#39;Hebei&#39;: 1, &#39;Heilongjiang&#39;: 1, &#39;Henan&#39;: 1, &#39;Hong Kong&#39;: 1, &#39;Hubei&#39;: 1, &#39;Hunan&#39;: 1, &#39;Inner Mongolia&#39;: 1, &#39;Jiangsu&#39;: 1, &#39;Jiangxi&#39;: 1, &#39;Jilin&#39;: 1, &#39;Liaoning&#39;: 1, &#39;Macau&#39;: 1, &#39;Ningxia&#39;: 1, &#39;Qinghai&#39;: 1, &#39;Shaanxi&#39;: 1, &#39;Shandong&#39;: 1, &#39;Shanghai&#39;: 1, &#39;Shanxi&#39;: 1, &#39;Sichuan&#39;: 1, &#39;Taiwan&#39;: 1, &#39;Tianjin&#39;: 1, &#39;Tibet&#39;: 1, &#39;Washington&#39;: 1, &#39;Xinjiang&#39;: 1, &#39;Yunnan&#39;: 1, &#39;Zhejiang&#39;: 1, nan: 11}) . list_of_all_files = os.listdir() . # list_of_all_files . actual_list = [] for i in list_of_all_files: if &quot;csv&quot; in i: actual_list.append(i) . main = [] province_list = [] country_list = [] confirmed = [] death_count = [] date_list = [] for j in tqdm(actual_list): a = pd.read_csv(j) for i in range(len(a)): date_list.append(&quot;Date: &quot; + str(j[:-4])) for i in a[&#39;Province/State&#39;]: province_list.append(i) for i in a[&#39;Country/Region&#39;]: country_list.append(i) for i in a[&#39;Confirmed&#39;]: confirmed.append(i) for i in a[&#39;Deaths&#39;]: death_count.append(i) . 100%|██████████| 61/61 [00:00&lt;00:00, 291.88it/s] . main_df = pd.DataFrame({ &#39;date&#39;: date_list, &#39;state&#39;: province_list, &#39;country&#39;:country_list, &#39;confirmed_case&#39;: confirmed, &#39;death&#39;: death_count }) . len(death_count) . 7926 . Counter(country_list)[&#39;Bangladesh&#39;] . 15 . total_confirmed_case = 0.0 case_of_bd = [] case_of_death_bd = [] date_of_bd = [] for index, row in main_df.iterrows(): if row[&#39;country&#39;] == &quot;Bangladesh&quot;: print(row[&#39;date&#39;], &quot; Confirmed Cases: &quot;, row[&#39;confirmed_case&#39;], &quot; Confirmed Death: &quot;, row[&#39;death&#39;]) total_confirmed_case += row[&#39;confirmed_case&#39;] case_of_bd.append(row[&#39;confirmed_case&#39;]) case_of_death_bd.append(row[&#39;death&#39;]) date_of_bd.append(row[&#39;date&#39;]) # print(total_confirmed_case) bd_df = pd.DataFrame({ &#39;date&#39;: date_of_bd, &#39;confirmed_case&#39;: case_of_bd, &#39;death&#39;: case_of_death_bd }) . Date: 03-08-2020 Confirmed Cases: 3.0 Confirmed Death: 0.0 Date: 03-09-2020 Confirmed Cases: 3.0 Confirmed Death: 0.0 Date: 03-10-2020 Confirmed Cases: 3.0 Confirmed Death: 0.0 Date: 03-11-2020 Confirmed Cases: 3.0 Confirmed Death: 0.0 Date: 03-12-2020 Confirmed Cases: 3.0 Confirmed Death: 0.0 Date: 03-13-2020 Confirmed Cases: 3.0 Confirmed Death: 0.0 Date: 03-14-2020 Confirmed Cases: 3.0 Confirmed Death: 0.0 Date: 03-15-2020 Confirmed Cases: 5.0 Confirmed Death: 0.0 Date: 03-16-2020 Confirmed Cases: 8.0 Confirmed Death: 0.0 Date: 03-17-2020 Confirmed Cases: 10.0 Confirmed Death: 0.0 Date: 03-18-2020 Confirmed Cases: 14.0 Confirmed Death: 1.0 Date: 03-19-2020 Confirmed Cases: 17.0 Confirmed Death: 1.0 Date: 03-20-2020 Confirmed Cases: 20.0 Confirmed Death: 1.0 Date: 03-21-2020 Confirmed Cases: 25.0 Confirmed Death: 2.0 Date: 03-22-2020 Confirmed Cases: 27.0 Confirmed Death: 2.0 . ax = plt.figure(figsize=(20,10)) # Add title ax = plt.title(&quot;Confirmed Cases in Bangladesh&quot;) ax = sns.barplot(y=bd_df[&#39;confirmed_case&#39;], x=bd_df[&#39;date&#39;], saturation = 0.4, color=&quot;red&quot;) # ax = sns.color_palette(&quot;RdBu&quot;, n_colors=7) ax.set_xticklabels(labels=bd_df[&#39;date&#39;], rotation=45, ha=&#39;right&#39;) ax = plt.ylabel(&quot;Count&quot;) . ax = plt.figure(figsize=(20,10)) # Add title ax = plt.title(&quot;Confirmed Cases and Death Count due to COVID-19 in Bangladesh&quot;) ax = sns.barplot(y=bd_df[&#39;confirmed_case&#39;], x=bd_df[&#39;date&#39;], saturation = 0.4, color = &#39;Blue&#39;, label = &quot;Confirmed Case&quot;) ax = sns.barplot(x = bd_df[&#39;date&#39;], y = bd_df[&#39;death&#39;], color = &#39;red&#39;, label = &quot;Death Count&quot;) ax.legend() # ax = sns.color_palette(&quot;RdBu&quot;, n_colors=7) ax.set_xticklabels(labels=bd_df[&#39;date&#39;], rotation=45, ha=&#39;right&#39;) ax = plt.ylabel(&quot;Count&quot;) . Counter(country_list).most_common(30) . [(&#39;US&#39;, 1617), (&#39;Mainland China&#39;, 1517), (&#39;China&#39;, 396), (&#39;Australia&#39;, 323), (&#39;Canada&#39;, 254), (&#39;France&#39;, 127), (&#39;Japan&#39;, 61), (&#39;Thailand&#39;, 61), (&#39;Singapore&#39;, 60), (&#39;Malaysia&#39;, 59), (&#39;Vietnam&#39;, 59), (&#39;Nepal&#39;, 58), (&#39;Cambodia&#39;, 56), (&#39;Sri Lanka&#39;, 56), (&#39;Germany&#39;, 55), (&#39;United Kingdom&#39;, 55), (&#39;United Arab Emirates&#39;, 54), (&#39;Philippines&#39;, 54), (&#39;Finland&#39;, 54), (&#39;India&#39;, 53), (&#39;Italy&#39;, 52), (&#39;Sweden&#39;, 52), (&#39;Russia&#39;, 51), (&#39;Spain&#39;, 51), (&#39;Hong Kong&#39;, 48), (&#39;South Korea&#39;, 48), (&#39;Taiwan&#39;, 48), (&#39;Macau&#39;, 48), (&#39;Belgium&#39;, 48), (&#39;Denmark&#39;, 41)] . def specific_country(search): # total_confirmed_case = 0.0 case_of_confirmed = [] case_of_death = [] date = [] state = [] for index, row in main_df.iterrows(): if row[&#39;country&#39;] == search: case_of_confirmed.append(row[&#39;confirmed_case&#39;]) case_of_death.append(row[&#39;death&#39;]) date.append(row[&#39;date&#39;]) state.append(row[&#39;state&#39;]) # print(total_confirmed_case) bd_df = pd.DataFrame({ &#39;date&#39;: date, &#39;confirmed_case&#39;: case_of_confirmed, &#39;death&#39;: case_of_death, &#39;state&#39;: state }) return bd_df . bd_df = specific_country(&quot;Bangladesh&quot;) . ax = plt.figure(figsize=(20,10)) # Add title ax = plt.title(&quot;Confirmed Cases and Death Count due to COVID-19 in Bangladesh&quot;) ax = sns.lineplot(y=bd_df[&#39;confirmed_case&#39;], x=bd_df[&#39;date&#39;], color = &#39;Blue&#39;, label = &quot;Confirmed Case&quot;) ax = sns.lineplot(x = bd_df[&#39;date&#39;], y = bd_df[&#39;death&#39;], color = &#39;red&#39;, label = &quot;Death Count&quot;) ax.legend() # ax = sns.color_palette(&quot;RdBu&quot;, n_colors=7) ax.set_xticklabels(labels=bd_df[&#39;date&#39;], rotation=45, ha=&#39;right&#39;) ax = plt.ylabel(&quot;Count&quot;) . plt.figure(figsize=(14,6)) # Add title plt.title(&quot;Rise of COVID-19 in Bangladesh&quot;) sns.lineplot(data=bd_df[&#39;confirmed_case&#39;], label=&quot;Confirmed Case&quot;) sns.lineplot(data=bd_df[&#39;death&#39;], label=&quot;Death Toll&quot;, color = &quot;red&quot;) # Add label for horizontal axis plt.xlabel(&quot;Date&quot;) . Text(0.5, 0, &#39;Date&#39;) . us_df = specific_country(&quot;US&quot;) us_df.head() . date confirmed_case death state . 0 Date: 02-07-2020 | 2.0 | 0.0 | Chicago, IL | . 1 Date: 02-07-2020 | 2.0 | 0.0 | San Benito, CA | . 2 Date: 02-07-2020 | 2.0 | 0.0 | Santa Clara, CA | . 3 Date: 02-07-2020 | 1.0 | 0.0 | Boston, MA | . 4 Date: 02-07-2020 | 1.0 | 0.0 | Los Angeles, CA | . Counter(list(us_df[&#39;state&#39;])) . Counter({&#39;Chicago, IL&#39;: 30, &#39;San Benito, CA&#39;: 36, &#39;Santa Clara, CA&#39;: 35, &#39;Boston, MA&#39;: 34, &#39;Los Angeles, CA&#39;: 38, &#39;Madison, WI&#39;: 34, &#39;Orange, CA&#39;: 32, &#39;Seattle, WA&#39;: 30, &#39;Tempe, AZ&#39;: 35, &#39;Unassigned Location (From Diamond Princess)&#39;: 15, &#39;San Diego County, CA&#39;: 28, &#39;Humboldt County, CA&#39;: 18, &#39;Sacramento County, CA&#39;: 18, &#39;San Antonio, TX&#39;: 26, &#39;Lackland, TX (From Diamond Princess)&#39;: 17, &#39;Omaha, NE (From Diamond Princess)&#39;: 17, &#39;Travis, CA (From Diamond Princess)&#39;: 17, &#39;Washington&#39;: 23, &#39;Chicago&#39;: 1, &#39;Illinois&#39;: 20, &#39;California&#39;: 19, &#39;Arizona&#39;: 19, &#39;Ashland, NE&#39;: 1, &#39;Travis, CA&#39;: 1, &#39;Lackland, TX&#39;: 1, &#39;Portland, OR&#39;: 3, &#39;Snohomish County, WA&#39;: 10, &#39;Providence, RI&#39;: 6, &#39;King County, WA&#39;: 8, &#39;Cook County, IL&#39;: 8, &#39;Grafton County, NH&#39;: 8, &#39;Hillsborough, FL&#39;: 8, &#39;New York City, NY&#39;: 4, &#39;Placer County, CA&#39;: 8, &#39;San Mateo, CA&#39;: 8, &#39;Sarasota, FL&#39;: 8, &#39;Sonoma County, CA&#39;: 8, &#39;Umatilla, OR&#39;: 8, &#39;Fulton County, GA&#39;: 7, &#39;Washington County, OR&#39;: 7, &#39; Norfolk County, MA&#39;: 5, &#39;Berkeley, CA&#39;: 4, &#39;Maricopa County, AZ&#39;: 7, &#39;Wake County, NC&#39;: 7, &#39;Westchester County, NY&#39;: 7, &#39;Orange County, CA&#39;: 6, &#39;Contra Costa County, CA&#39;: 6, &#39;Bergen County, NJ&#39;: 5, &#39;Harris County, TX&#39;: 5, &#39;San Francisco County, CA&#39;: 5, &#39;Clark County, NV&#39;: 5, &#39;Fort Bend County, TX&#39;: 5, &#39;Grant County, WA&#39;: 5, &#39;Queens County, NY&#39;: 1, &#39;Santa Rosa County, FL&#39;: 5, &#39;Williamson County, TN&#39;: 5, &#39;New York County, NY&#39;: 4, &#39;Unassigned Location, WA&#39;: 4, &#39;Montgomery County, MD&#39;: 4, &#39;Suffolk County, MA&#39;: 4, &#39;Denver County, CO&#39;: 4, &#39;Summit County, CO&#39;: 4, &#39;Chatham County, NC&#39;: 4, &#39;Delaware County, PA&#39;: 4, &#39;Douglas County, NE&#39;: 4, &#39;Fayette County, KY&#39;: 4, &#39;Floyd County, GA&#39;: 2, &#39;Marion County, IN&#39;: 4, &#39;Middlesex County, MA&#39;: 4, &#39;Nassau County, NY&#39;: 4, &#39;Norwell County, MA&#39;: 1, &#39;Ramsey County, MN&#39;: 4, &#39;Washoe County, NV&#39;: 4, &#39;Wayne County, PA&#39;: 4, &#39;Yolo County, CA&#39;: 4, &#39;Santa Clara County, CA&#39;: 3, &#39;Grand Princess Cruise Ship&#39;: 3, &#39;Douglas County, CO&#39;: 3, &#39;Providence County, RI&#39;: 3, &#39;Alameda County, CA&#39;: 3, &#39;Broward County, FL&#39;: 3, &#39;Fairfield County, CT&#39;: 3, &#39;Lee County, FL&#39;: 3, &#39;Pinal County, AZ&#39;: 3, &#39;Rockland County, NY&#39;: 3, &#39;Saratoga County, NY&#39;: 3, &#39;Charleston County, SC&#39;: 3, &#39;Clark County, WA&#39;: 3, &#39;Cobb County, GA&#39;: 3, &#39;Davis County, UT&#39;: 3, &#39;El Paso County, CO&#39;: 3, &#39;Honolulu County, HI&#39;: 3, &#39;Jackson County, OR &#39;: 3, &#39;Jefferson County, WA&#39;: 3, &#39;Kershaw County, SC&#39;: 3, &#39;Klamath County, OR&#39;: 3, &#39;Madera County, CA&#39;: 3, &#39;Pierce County, WA&#39;: 3, &#39;Plymouth County, MA&#39;: 3, &#39;Santa Cruz County, CA&#39;: 1, &#39;Tulsa County, OK&#39;: 3, &#39;Montgomery County, TX&#39;: 3, &#39;Norfolk County, MA&#39;: 2, &#39;Montgomery County, PA&#39;: 2, &#39;Fairfax County, VA&#39;: 2, &#39;Rockingham County, NH&#39;: 2, &#39;Washington, D.C.&#39;: 2, &#39;Berkshire County, MA&#39;: 2, &#39;Davidson County, TN&#39;: 2, &#39;Douglas County, OR&#39;: 2, &#39;Fresno County, CA&#39;: 2, &#39;Harford County, MD&#39;: 2, &#39;Hendricks County, IN&#39;: 2, &#39;Hudson County, NJ&#39;: 2, &#39;Johnson County, KS&#39;: 2, &#39;Kittitas County, WA&#39;: 2, &#39;Manatee County, FL&#39;: 2, &#39;Marion County, OR&#39;: 2, &#39;Okaloosa County, FL&#39;: 2, &#39;Polk County, GA&#39;: 2, &#39;Riverside County, CA&#39;: 2, &#39;Shelby County, TN&#39;: 2, &#39;Spokane County, WA&#39;: 2, &#39;St. Louis County, MO&#39;: 2, &#39;Suffolk County, NY&#39;: 2, &#39;Ulster County, NY&#39;: 2, &#39;Unassigned Location, VT&#39;: 1, &#39;Unknown Location, MA&#39;: 2, &#39;Volusia County, FL&#39;: 2, &#39;Johnson County, IA&#39;: 1, &#39;Harrison County, KY&#39;: 1, &#39;Bennington County, VT&#39;: 1, &#39;Carver County, MN&#39;: 1, &#39;Charlotte County, FL&#39;: 1, &#39;Cherokee County, GA&#39;: 1, &#39;Collin County, TX&#39;: 1, &#39;Jefferson County, KY&#39;: 1, &#39;Jefferson Parish, LA&#39;: 1, &#39;Shasta County, CA&#39;: 1, &#39;Spartanburg County, SC&#39;: 1, &#39;New York&#39;: 13, &#39;Massachusetts&#39;: 13, &#39;Diamond Princess&#39;: 13, &#39;Grand Princess&#39;: 13, &#39;Georgia&#39;: 13, &#39;Colorado&#39;: 13, &#39;Florida&#39;: 13, &#39;New Jersey&#39;: 13, &#39;Oregon&#39;: 13, &#39;Texas&#39;: 13, &#39;Pennsylvania&#39;: 13, &#39;Iowa&#39;: 13, &#39;Maryland&#39;: 13, &#39;North Carolina&#39;: 13, &#39;South Carolina&#39;: 13, &#39;Tennessee&#39;: 13, &#39;Virginia&#39;: 13, &#39;Indiana&#39;: 13, &#39;Kentucky&#39;: 13, &#39;District of Columbia&#39;: 13, &#39;Nevada&#39;: 13, &#39;New Hampshire&#39;: 13, &#39;Minnesota&#39;: 13, &#39;Nebraska&#39;: 13, &#39;Ohio&#39;: 13, &#39;Rhode Island&#39;: 13, &#39;Wisconsin&#39;: 13, &#39;Connecticut&#39;: 13, &#39;Hawaii&#39;: 13, &#39;Oklahoma&#39;: 13, &#39;Utah&#39;: 13, &#39;Kansas&#39;: 13, &#39;Louisiana&#39;: 13, &#39;Missouri&#39;: 13, &#39;Vermont&#39;: 13, &#39;Alaska&#39;: 12, &#39;Arkansas&#39;: 13, &#39;Delaware&#39;: 13, &#39;Idaho&#39;: 13, &#39;Maine&#39;: 13, &#39;Michigan&#39;: 13, &#39;Mississippi&#39;: 13, &#39;Montana&#39;: 13, &#39;New Mexico&#39;: 13, &#39;North Dakota&#39;: 13, &#39;South Dakota&#39;: 13, &#39;West Virginia&#39;: 13, &#39;Wyoming&#39;: 13, &#39;Alabama&#39;: 10, &#39;Puerto Rico&#39;: 9, &#39;Virgin Islands, U.S.&#39;: 2, &#39;Guam&#39;: 8, &#39;Virgin Islands&#39;: 4, &#39;United States Virgin Islands&#39;: 5, &#39;US&#39;: 5}) . duplicates_included = list(us_df[&#39;state&#39;]) . duplicate_dates = list(us_df[&#39;date&#39;]) . h = duplicate_dates[6] . if h not in duplicate_dates: pass else: print(&quot;no&quot;) . no . all_dates = [] for x in duplicate_dates: if x not in all_dates: all_dates.append(x) . len(all_dates) . 61 . all_states = [] for x in duplicates_included: if x not in all_states: all_states.append(x) . new_df = pd.DataFrame() a_dictionary = {} for i in all_states: case_of_confirmed = [] date_list = [] for index, row in tqdm(us_df.iterrows()): if i == row[&#39;state&#39;]: case_of_confirmed.append(row[&#39;confirmed_case&#39;]) date_list.append(row[&#39;date&#39;]) a_dictionary[i] = case_of_confirmed . 1617it [00:00, 9897.03it/s] 1617it [00:00, 11750.16it/s] 1617it [00:00, 11742.24it/s] 1617it [00:00, 11434.92it/s] 1617it [00:00, 11893.80it/s] 1617it [00:00, 10751.23it/s] 1617it [00:00, 10138.73it/s] 1617it [00:00, 9356.25it/s] 1617it [00:00, 10347.46it/s] 1617it [00:00, 11522.09it/s] 1617it [00:00, 10398.69it/s] 1617it [00:00, 10219.87it/s] 1617it [00:00, 9235.74it/s] 1617it [00:00, 9264.53it/s] 1617it [00:00, 9003.03it/s] 1617it [00:00, 9227.36it/s] 1617it [00:00, 9227.12it/s] 1617it [00:00, 9034.51it/s] 1617it [00:00, 8195.64it/s] 1617it [00:00, 8527.91it/s] 1617it [00:00, 8160.34it/s] 1617it [00:00, 8660.94it/s] 1617it [00:00, 8609.87it/s] 1617it [00:00, 7859.78it/s] 1617it [00:00, 8486.59it/s] 1617it [00:00, 9129.04it/s] 1617it [00:00, 9170.49it/s] 1617it [00:00, 8998.69it/s] 1617it [00:00, 8724.91it/s] 1617it [00:00, 9067.24it/s] 1617it [00:00, 8660.79it/s] 1617it [00:00, 9139.96it/s] 1617it [00:00, 9242.53it/s] 1617it [00:00, 8815.31it/s] 1617it [00:00, 8323.04it/s] 1617it [00:00, 9277.44it/s] 1617it [00:00, 9151.53it/s] 1617it [00:00, 9443.72it/s] 1617it [00:00, 9101.01it/s] 1617it [00:00, 9480.81it/s] 1617it [00:00, 8805.89it/s] 1617it [00:00, 8654.21it/s] 1617it [00:00, 8923.72it/s] 1617it [00:00, 6763.55it/s] 1617it [00:00, 9370.28it/s] 1617it [00:00, 9316.31it/s] 1617it [00:00, 9550.51it/s] 1617it [00:00, 9002.05it/s] 1617it [00:00, 9469.38it/s] 1617it [00:00, 9253.00it/s] 1617it [00:00, 8832.69it/s] 1617it [00:00, 8076.19it/s] 1617it [00:00, 9120.41it/s] 1617it [00:00, 9088.79it/s] 1617it [00:00, 8280.46it/s] 1617it [00:00, 8369.44it/s] 1617it [00:00, 7817.37it/s] 1617it [00:00, 9069.88it/s] 1617it [00:00, 9476.14it/s] 1617it [00:00, 9548.61it/s] 1617it [00:00, 8863.21it/s] 1617it [00:00, 7681.98it/s] 1617it [00:00, 8134.70it/s] 1617it [00:00, 8906.50it/s] 1617it [00:00, 9616.28it/s] 1617it [00:00, 9422.23it/s] 1617it [00:00, 8871.82it/s] 1617it [00:00, 9173.40it/s] 1617it [00:00, 9087.24it/s] 1617it [00:00, 9593.06it/s] 1617it [00:00, 9458.01it/s] 1617it [00:00, 9704.04it/s] 1617it [00:00, 8506.53it/s] 1617it [00:00, 8809.24it/s] 1617it [00:00, 9324.11it/s] 1617it [00:00, 9138.52it/s] 1617it [00:00, 9295.73it/s] 1617it [00:00, 9470.56it/s] 1617it [00:00, 8840.73it/s] 1617it [00:00, 9301.09it/s] 1617it [00:00, 9254.12it/s] 1617it [00:00, 9076.39it/s] 1617it [00:00, 9507.97it/s] 1617it [00:00, 8809.27it/s] 1617it [00:00, 9387.24it/s] 1617it [00:00, 9530.12it/s] 1617it [00:00, 8957.30it/s] 1617it [00:00, 9329.12it/s] 1617it [00:00, 9365.45it/s] 1617it [00:00, 8436.51it/s] 1617it [00:00, 9220.87it/s] 1617it [00:00, 9055.25it/s] 1617it [00:00, 9792.71it/s] 1617it [00:00, 8957.36it/s] 1617it [00:00, 9159.69it/s] 1617it [00:00, 8534.27it/s] 1617it [00:00, 9510.67it/s] 1617it [00:00, 9482.52it/s] 1617it [00:00, 9440.95it/s] 1617it [00:00, 9413.64it/s] 1617it [00:00, 8774.43it/s] 1617it [00:00, 8972.99it/s] 1617it [00:00, 9310.22it/s] 1617it [00:00, 9486.76it/s] 1617it [00:00, 9711.50it/s] 1617it [00:00, 9616.58it/s] 1617it [00:00, 8461.64it/s] 1617it [00:00, 9290.35it/s] 1617it [00:00, 9442.05it/s] 1617it [00:00, 9490.33it/s] 1617it [00:00, 8717.96it/s] 1617it [00:00, 8403.14it/s] 1617it [00:00, 7958.63it/s] 1617it [00:00, 8583.46it/s] 1617it [00:00, 8710.26it/s] 1617it [00:00, 8792.19it/s] 1617it [00:00, 9056.12it/s] 1617it [00:00, 8133.16it/s] 1617it [00:00, 9167.86it/s] 1617it [00:00, 8401.98it/s] 1617it [00:00, 8888.65it/s] 1617it [00:00, 9304.73it/s] 1617it [00:00, 8224.59it/s] 1617it [00:00, 9238.79it/s] 1617it [00:00, 8862.06it/s] 1617it [00:00, 9083.52it/s] 1617it [00:00, 8734.28it/s] 1617it [00:00, 9274.50it/s] 1617it [00:00, 9002.36it/s] 1617it [00:00, 9638.61it/s] 1617it [00:00, 9430.83it/s] 1617it [00:00, 9472.25it/s] 1617it [00:00, 9332.68it/s] 1617it [00:00, 8101.39it/s] 1617it [00:00, 7268.92it/s] 1617it [00:00, 9203.90it/s] 1617it [00:00, 9422.46it/s] 1617it [00:00, 9346.25it/s] 1617it [00:00, 9427.41it/s] 1617it [00:00, 8463.77it/s] 1617it [00:00, 9705.91it/s] 1617it [00:00, 9152.91it/s] 1617it [00:00, 9367.32it/s] 1617it [00:00, 9272.37it/s] 1617it [00:00, 8533.95it/s] 1617it [00:00, 8920.42it/s] 1617it [00:00, 8939.04it/s] 1617it [00:00, 8863.05it/s] 1617it [00:00, 8606.84it/s] 1617it [00:00, 8288.81it/s] 1617it [00:00, 8476.75it/s] 1617it [00:00, 8776.90it/s] 1617it [00:00, 9467.37it/s] 1617it [00:00, 9579.38it/s] 1617it [00:00, 9405.31it/s] 1617it [00:00, 8639.31it/s] 1617it [00:00, 8596.24it/s] 1617it [00:00, 8538.37it/s] 1617it [00:00, 9105.70it/s] 1617it [00:00, 8869.80it/s] 1617it [00:00, 7665.70it/s] 1617it [00:00, 9406.62it/s] 1617it [00:00, 9726.43it/s] 1617it [00:00, 9417.72it/s] 1617it [00:00, 9162.55it/s] 1617it [00:00, 9288.11it/s] 1617it [00:00, 9183.26it/s] 1617it [00:00, 9149.00it/s] 1617it [00:00, 8791.07it/s] 1617it [00:00, 8842.39it/s] 1617it [00:00, 9279.17it/s] 1617it [00:00, 9389.50it/s] 1617it [00:00, 8657.41it/s] 1617it [00:00, 9049.60it/s] 1617it [00:00, 9675.05it/s] 1617it [00:00, 9367.64it/s] 1617it [00:00, 9238.52it/s] 1617it [00:00, 7418.38it/s] 1617it [00:00, 5746.97it/s] 1617it [00:00, 7992.78it/s] 1617it [00:00, 6606.99it/s] 1617it [00:00, 9130.24it/s] 1617it [00:00, 8933.36it/s] 1617it [00:00, 8811.96it/s] 1617it [00:00, 9018.10it/s] 1617it [00:00, 9130.47it/s] 1617it [00:00, 9236.86it/s] 1617it [00:00, 7730.59it/s] 1617it [00:00, 9486.96it/s] 1617it [00:00, 8474.33it/s] 1617it [00:00, 9344.43it/s] 1617it [00:00, 9065.39it/s] 1617it [00:00, 9025.59it/s] 1617it [00:00, 9061.02it/s] 1617it [00:00, 9350.40it/s] . # a_dictionary . length_of_lists = [] for key, value in a_dictionary.items(): a = len(value) length_of_lists.append(a) . maximum = max(length_of_lists) . maximum = 61 . state_counter = [] cases_counter = [] for key, value in a_dictionary.items(): if len(value) == 0: new = [0] * maximum value = new else: new = value + [value[-1]] * (maximum-len(value)) value = new state_counter.append(key) cases_counter.append(value) . cases_counter[0] . [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0] . new_df = pd.DataFrame() . # new_df[&#39;Date&#39;] = all_dates . for f, b in zip(state_counter, cases_counter): new_df[f] = b . new_df . Chicago, IL San Benito, CA Santa Clara, CA Boston, MA Los Angeles, CA Madison, WI Orange, CA Seattle, WA Tempe, AZ Unassigned Location (From Diamond Princess) ... South Dakota West Virginia Wyoming Alabama Puerto Rico Virgin Islands, U.S. Guam Virgin Islands United States Virgin Islands US . 0 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 36.0 | ... | 0.0 | 0.0 | 0.0 | 5.0 | 3.0 | 1.0 | 3.0 | 1.0 | 2.0 | 1.0 | . 1 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 36.0 | ... | 8.0 | 0.0 | 0.0 | 6.0 | 5.0 | 1.0 | 3.0 | 2.0 | 2.0 | 1.0 | . 2 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 42.0 | ... | 8.0 | 0.0 | 1.0 | 12.0 | 5.0 | 1.0 | 3.0 | 2.0 | 3.0 | 1.0 | . 3 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 42.0 | ... | 8.0 | 0.0 | 1.0 | 29.0 | 5.0 | 1.0 | 5.0 | 3.0 | 6.0 | 1.0 | . 4 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 44.0 | ... | 9.0 | 0.0 | 2.0 | 39.0 | 5.0 | 1.0 | 12.0 | 3.0 | 6.0 | 1.0 | . 5 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 44.0 | ... | 9.0 | 0.0 | 3.0 | 46.0 | 5.0 | 1.0 | 14.0 | 3.0 | 6.0 | 1.0 | . 6 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 44.0 | ... | 10.0 | 0.0 | 3.0 | 78.0 | 14.0 | 1.0 | 15.0 | 3.0 | 6.0 | 1.0 | . 7 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 11.0 | 1.0 | 11.0 | 83.0 | 21.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 8 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 11.0 | 1.0 | 15.0 | 131.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 9 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 11.0 | 2.0 | 18.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 10 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 14.0 | 7.0 | 19.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 11 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 14.0 | 8.0 | 23.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 12 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 13 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 14 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 15 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 16 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 17 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 18 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 19 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 20 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 21 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 22 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 23 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 24 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 25 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 26 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 27 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 28 2.0 | 2.0 | 3.0 | 1.0 | 1.0 | 1.0 | 1.0 | 6.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 29 3.0 | 2.0 | 3.0 | 1.0 | 1.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 31 3.0 | 2.0 | 11.0 | 1.0 | 1.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 32 3.0 | 2.0 | 11.0 | 1.0 | 7.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 33 3.0 | 2.0 | 20.0 | 1.0 | 11.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 34 3.0 | 2.0 | 20.0 | 1.0 | 13.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 35 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 36 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 37 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 38 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 39 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 40 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 41 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 42 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 43 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 44 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 45 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 46 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 47 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 48 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 49 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 50 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 51 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 52 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 53 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 54 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 55 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 56 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 57 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 58 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 59 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 60 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 61 rows × 195 columns . new_date_df = pd.DataFrame() . new_date_df[&quot;Date&quot;] = all_dates . u = [] for i in new_df: print(i) . Chicago, IL San Benito, CA Santa Clara, CA Boston, MA Los Angeles, CA Madison, WI Orange, CA Seattle, WA Tempe, AZ Unassigned Location (From Diamond Princess) San Diego County, CA Humboldt County, CA Sacramento County, CA San Antonio, TX Lackland, TX (From Diamond Princess) Omaha, NE (From Diamond Princess) Travis, CA (From Diamond Princess) Washington Chicago Illinois California Arizona Ashland, NE Travis, CA Lackland, TX Portland, OR Snohomish County, WA Providence, RI King County, WA Cook County, IL Grafton County, NH Hillsborough, FL New York City, NY Placer County, CA San Mateo, CA Sarasota, FL Sonoma County, CA Umatilla, OR Fulton County, GA Washington County, OR Norfolk County, MA Berkeley, CA Maricopa County, AZ Wake County, NC Westchester County, NY Orange County, CA Contra Costa County, CA Bergen County, NJ Harris County, TX San Francisco County, CA Clark County, NV Fort Bend County, TX Grant County, WA Queens County, NY Santa Rosa County, FL Williamson County, TN New York County, NY Unassigned Location, WA Montgomery County, MD Suffolk County, MA Denver County, CO Summit County, CO Chatham County, NC Delaware County, PA Douglas County, NE Fayette County, KY Floyd County, GA Marion County, IN Middlesex County, MA Nassau County, NY Norwell County, MA Ramsey County, MN Washoe County, NV Wayne County, PA Yolo County, CA Santa Clara County, CA Grand Princess Cruise Ship Douglas County, CO Providence County, RI Alameda County, CA Broward County, FL Fairfield County, CT Lee County, FL Pinal County, AZ Rockland County, NY Saratoga County, NY Charleston County, SC Clark County, WA Cobb County, GA Davis County, UT El Paso County, CO Honolulu County, HI Jackson County, OR Jefferson County, WA Kershaw County, SC Klamath County, OR Madera County, CA Pierce County, WA Plymouth County, MA Santa Cruz County, CA Tulsa County, OK Montgomery County, TX Norfolk County, MA Montgomery County, PA Fairfax County, VA Rockingham County, NH Washington, D.C. Berkshire County, MA Davidson County, TN Douglas County, OR Fresno County, CA Harford County, MD Hendricks County, IN Hudson County, NJ Johnson County, KS Kittitas County, WA Manatee County, FL Marion County, OR Okaloosa County, FL Polk County, GA Riverside County, CA Shelby County, TN Spokane County, WA St. Louis County, MO Suffolk County, NY Ulster County, NY Unassigned Location, VT Unknown Location, MA Volusia County, FL Johnson County, IA Harrison County, KY Bennington County, VT Carver County, MN Charlotte County, FL Cherokee County, GA Collin County, TX Jefferson County, KY Jefferson Parish, LA Shasta County, CA Spartanburg County, SC New York Massachusetts Diamond Princess Grand Princess Georgia Colorado Florida New Jersey Oregon Texas Pennsylvania Iowa Maryland North Carolina South Carolina Tennessee Virginia Indiana Kentucky District of Columbia Nevada New Hampshire Minnesota Nebraska Ohio Rhode Island Wisconsin Connecticut Hawaii Oklahoma Utah Kansas Louisiana Missouri Vermont Alaska Arkansas Delaware Idaho Maine Michigan Mississippi Montana New Mexico North Dakota South Dakota West Virginia Wyoming Alabama Puerto Rico Virgin Islands, U.S. Guam Virgin Islands United States Virgin Islands US . ax = plt.figure(figsize=(20,6)) # Add title ax = plt.title(&quot;Confirmed Cases and Death Count due to COVID-19 in USA&quot;) for i in tqdm(new_df): ax = sns.lineplot(x=new_date_df[&#39;Date&#39;], y=new_df[i], label = str(i)) # ax = sns.lineplot(x = bd_df[&#39;date&#39;], y = bd_df[&#39;death&#39;], color = &#39;red&#39;, label = &quot;Death Count&quot;) ax.legend() # ax = sns.color_palette(&quot;RdBu&quot;, n_colors=7) ax.set_xticklabels(labels=new_date_df[&#39;Date&#39;], rotation=45, ha=&#39;right&#39;) ax = plt.ylabel(&quot;Count&quot;) . 0%| | 0/61 [00:00&lt;?, ?it/s] 2%|▏ | 1/61 [00:00&lt;00:07, 8.31it/s] 8%|▊ | 5/61 [00:00&lt;00:05, 10.68it/s] 13%|█▎ | 8/61 [00:00&lt;00:04, 13.14it/s] 18%|█▊ | 11/61 [00:00&lt;00:03, 15.61it/s] 23%|██▎ | 14/61 [00:00&lt;00:02, 17.79it/s] 28%|██▊ | 17/61 [00:00&lt;00:02, 19.44it/s] 33%|███▎ | 20/61 [00:00&lt;00:02, 20.44it/s] 38%|███▊ | 23/61 [00:00&lt;00:01, 21.23it/s] 43%|████▎ | 26/61 [00:01&lt;00:01, 21.29it/s] 48%|████▊ | 29/61 [00:01&lt;00:01, 20.95it/s] 52%|█████▏ | 32/61 [00:01&lt;00:01, 20.67it/s] 57%|█████▋ | 35/61 [00:01&lt;00:01, 19.36it/s] 61%|██████ | 37/61 [00:01&lt;00:01, 18.97it/s] 64%|██████▍ | 39/61 [00:01&lt;00:01, 17.95it/s] 67%|██████▋ | 41/61 [00:01&lt;00:01, 17.84it/s] 70%|███████ | 43/61 [00:02&lt;00:01, 17.70it/s] 74%|███████▍ | 45/61 [00:02&lt;00:00, 16.85it/s] 77%|███████▋ | 47/61 [00:02&lt;00:00, 16.10it/s] 80%|████████ | 49/61 [00:02&lt;00:00, 16.15it/s] 84%|████████▎ | 51/61 [00:02&lt;00:00, 16.10it/s] 87%|████████▋ | 53/61 [00:02&lt;00:00, 16.08it/s] 90%|█████████ | 55/61 [00:02&lt;00:00, 15.35it/s] 93%|█████████▎| 57/61 [00:02&lt;00:00, 14.51it/s] 97%|█████████▋| 59/61 [00:03&lt;00:00, 13.43it/s] 100%|██████████| 61/61 [00:03&lt;00:00, 13.46it/s] 63it [00:03, 12.39it/s] 65it [00:03, 11.54it/s] 67it [00:03, 11.20it/s] 69it [00:04, 11.75it/s] 71it [00:04, 12.13it/s] 73it [00:05, 3.56it/s] 75it [00:05, 4.53it/s] 77it [00:05, 5.63it/s] 79it [00:06, 6.77it/s] 81it [00:06, 7.88it/s] 83it [00:06, 8.50it/s] 85it [00:06, 9.16it/s] 87it [00:07, 7.20it/s] 89it [00:07, 8.00it/s] 91it [00:07, 8.84it/s] 93it [00:07, 9.51it/s] 95it [00:07, 9.66it/s] 97it [00:07, 10.09it/s] 99it [00:08, 10.28it/s] 101it [00:08, 7.32it/s] 103it [00:08, 8.00it/s] 104it [00:08, 8.01it/s] 105it [00:09, 8.42it/s] 106it [00:09, 8.75it/s] 107it [00:09, 8.99it/s] 108it [00:09, 8.59it/s] 109it [00:09, 7.74it/s] 110it [00:09, 7.68it/s] 111it [00:09, 6.85it/s] 112it [00:10, 4.62it/s] 113it [00:10, 5.10it/s] 114it [00:10, 5.45it/s] 115it [00:10, 5.98it/s] 116it [00:10, 6.09it/s] 117it [00:11, 5.85it/s] 118it [00:11, 6.08it/s] 119it [00:11, 6.12it/s] 120it [00:11, 6.35it/s] 121it [00:11, 6.71it/s] 122it [00:11, 6.32it/s] 123it [00:11, 6.61it/s] 124it [00:12, 3.93it/s] 125it [00:12, 4.38it/s] 126it [00:12, 5.00it/s] 127it [00:12, 5.43it/s] 128it [00:12, 6.14it/s] 129it [00:13, 6.73it/s] 130it [00:13, 7.28it/s] 131it [00:13, 7.47it/s] 132it [00:13, 7.43it/s] 133it [00:13, 7.56it/s] 134it [00:13, 7.57it/s] 135it [00:13, 7.72it/s] 136it [00:14, 4.26it/s] 137it [00:14, 4.89it/s] 138it [00:14, 5.39it/s] 139it [00:14, 5.89it/s] 140it [00:14, 6.31it/s] 141it [00:15, 6.48it/s] 142it [00:15, 6.70it/s] 143it [00:15, 6.99it/s] 144it [00:15, 6.97it/s] 145it [00:15, 6.75it/s] 146it [00:15, 7.10it/s] 147it [00:15, 7.05it/s] 148it [00:16, 3.83it/s] 149it [00:16, 4.51it/s] 150it [00:16, 5.13it/s] 151it [00:16, 5.61it/s] 152it [00:16, 5.81it/s] 153it [00:17, 6.11it/s] 154it [00:17, 6.48it/s] 155it [00:17, 6.56it/s] 156it [00:17, 6.71it/s] 157it [00:17, 6.75it/s] 158it [00:17, 6.65it/s] 159it [00:17, 6.79it/s] 160it [00:18, 3.70it/s] 161it [00:18, 4.39it/s] 162it [00:18, 4.95it/s] 163it [00:18, 5.46it/s] 164it [00:19, 5.88it/s] 165it [00:19, 6.33it/s] 166it [00:19, 6.70it/s] 167it [00:19, 6.94it/s] 168it [00:19, 7.16it/s] 169it [00:19, 7.23it/s] 170it [00:19, 7.02it/s] 171it [00:20, 7.12it/s] 172it [00:20, 3.66it/s] 173it [00:20, 4.26it/s] 174it [00:20, 4.84it/s] 175it [00:21, 5.36it/s] 176it [00:21, 5.60it/s] 177it [00:21, 5.71it/s] 178it [00:21, 5.86it/s] 179it [00:21, 6.20it/s] 180it [00:21, 6.39it/s] 181it [00:21, 6.48it/s] 182it [00:22, 6.32it/s] 183it [00:22, 6.27it/s] 184it [00:22, 3.33it/s] 185it [00:23, 3.95it/s] 186it [00:23, 4.54it/s] 187it [00:23, 5.06it/s] 188it [00:23, 5.43it/s] 189it [00:23, 5.71it/s] 190it [00:23, 5.90it/s] 191it [00:23, 6.10it/s] 192it [00:24, 6.22it/s] 193it [00:24, 6.34it/s] 194it [00:24, 6.47it/s] 195it [00:25, 3.25it/s] . ax = plt.figure(figsize=(20,6)) # Add title ax = plt.title(&quot;Confirmed Cases of COVID-19 in Chicago&quot;) # for i in tqdm(new_df): # ax = sns.lineplot(x=new_date_df[&#39;Date&#39;], y=new_df[i], label = str(i)) ax = sns.lineplot(x = new_date_df[&#39;Date&#39;], y = new_df[&#39;Chicago, IL&#39;], color = &#39;red&#39;, label = &quot;Confirmed Cases&quot;) ax.legend() # ax = sns.color_palette(&quot;RdBu&quot;, n_colors=7) ax.set_xticklabels(labels=new_date_df[&#39;Date&#39;], rotation=45, ha=&#39;right&#39;) ax = plt.ylabel(&quot;Count&quot;) .",
            "url": "https://thedrowsywinger.github.io/Analyzing-COVID-19-Data/2020/02/20/JHU-dataset.html",
            "relUrl": "/2020/02/20/JHU-dataset.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Title",
            "content": "EDA . import pandas as pd import seaborn as sns import json import matplotlib.pyplot as plt from tqdm import tqdm from collections import Counter import re from nltk.corpus import stopwords import seaborn as sns . reading_metadata = pd.read_csv(&quot;/media/thedrowsywinger/2A24A59224A56195/Poralekha/kaggle/covid/latter/CORD-19-research-challenge/metadata.csv&quot;) . title_list = [] for i in reading_metadata[&#39;title&#39;]: title_list.append(i) . len(title_list) . 44220 . words_list = [] for i in tqdm(title_list): # print(i) words = str(i).split(&quot; &quot;) for j in words: words_list.append(j.lower()) . 100%|██████████| 44220/44220 [00:00&lt;00:00, 232535.43it/s] . print(&#39;Number of words in text file :&#39;, len(words_list)) . Number of words in text file : 545436 . stop_words = set(stopwords.words(&#39;english&#39;)) filtered = [] for w in tqdm(words_list): if w not in stop_words: filtered.append(w) . 100%|██████████| 545436/545436 [00:00&lt;00:00, 1808284.80it/s] . frequency_dictionary = {} for keys in tqdm(filtered): frequency_dictionary[keys] = frequency_dictionary.get(keys, 0) + 1 . 100%|██████████| 401569/401569 [00:00&lt;00:00, 1477269.11it/s] . word_counts = Counter(filtered) w = word_counts.most_common(10) . list_of_words = [] list_of_counts = [] for i in w: list_of_words.append(i[0]) list_of_counts.append(i[1]) . title_df = pd.DataFrame({ &#39;text&#39;: list_of_words, &#39;count&#39;: list_of_counts }) . plt.figure(figsize=(20,6)) # Add title plt.title(&quot;Most Frequent Words in the title of the Research Papers Published&quot;) sns.barplot(x=title_df[&#39;text&#39;], y=title_df[&#39;count&#39;]) plt.ylabel(&quot;Count&quot;) . Text(0, 0.5, &#39;Count&#39;) .",
            "url": "https://thedrowsywinger.github.io/Analyzing-COVID-19-Data/2020/02/20/COVID-dataset.html",
            "relUrl": "/2020/02/20/COVID-dataset.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://thedrowsywinger.github.io/Analyzing-COVID-19-Data/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://thedrowsywinger.github.io/Analyzing-COVID-19-Data/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}