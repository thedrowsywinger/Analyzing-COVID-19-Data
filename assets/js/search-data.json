{
  
    
        "post0": {
            "title": "Title",
            "content": "This dataset has been taken from: https://github.com/CSSEGISandData/COVID-19 . import pandas as pd import os from collections import Counter from tqdm import tqdm %matplotlib inline import matplotlib.pyplot as plt import seaborn as sns . cd /media/thedrowsywinger/2A24A59224A56195/Poralekha/github/COVID-19/csse_covid_19_data/csse_covid_19_daily_reports . /media/thedrowsywinger/2A24A59224A56195/Poralekha/github/COVID-19/csse_covid_19_data/csse_covid_19_daily_reports . ls . 01-22-2020.csv* 02-07-2020.csv* 02-23-2020.csv* 03-10-2020.csv* 01-23-2020.csv* 02-08-2020.csv* 02-24-2020.csv* 03-11-2020.csv* 01-24-2020.csv* 02-09-2020.csv* 02-25-2020.csv* 03-12-2020.csv* 01-25-2020.csv* 02-10-2020.csv* 02-26-2020.csv* 03-13-2020.csv* 01-26-2020.csv* 02-11-2020.csv* 02-27-2020.csv* 03-14-2020.csv* 01-27-2020.csv* 02-12-2020.csv* 02-28-2020.csv* 03-15-2020.csv* 01-28-2020.csv* 02-13-2020.csv* 02-29-2020.csv* 03-16-2020.csv* 01-29-2020.csv* 02-14-2020.csv* 03-01-2020.csv* 03-17-2020.csv* 01-30-2020.csv* 02-15-2020.csv* 03-02-2020.csv* 03-18-2020.csv* 01-31-2020.csv* 02-16-2020.csv* 03-03-2020.csv* 03-19-2020.csv* 02-01-2020.csv* 02-17-2020.csv* 03-04-2020.csv* 03-20-2020.csv* 02-02-2020.csv* 02-18-2020.csv* 03-05-2020.csv* 03-21-2020.csv* 02-03-2020.csv* 02-19-2020.csv* 03-06-2020.csv* 03-22-2020.csv* 02-04-2020.csv* 02-20-2020.csv* 03-07-2020.csv* README.md* 02-05-2020.csv* 02-21-2020.csv* 03-08-2020.csv* 02-06-2020.csv* 02-22-2020.csv* 03-09-2020.csv* . a = pd.read_csv(&quot;/media/thedrowsywinger/2A24A59224A56195/Poralekha/github/COVID-19/csse_covid_19_data/csse_covid_19_daily_reports/01-23-2020.csv&quot;) . b = pd.read_csv(&quot;./02-25-2020.csv&quot;) . b . Province/State Country/Region Last Update Confirmed Deaths Recovered . 0 Hubei | Mainland China | 2020-02-25T15:23:04 | 64786 | 2563 | 18971 | . 1 Guangdong | Mainland China | 2020-02-25T08:53:02 | 1347 | 7 | 822 | . 2 Henan | Mainland China | 2020-02-25T12:43:02 | 1271 | 19 | 1002 | . 3 Zhejiang | Mainland China | 2020-02-25T09:13:05 | 1205 | 1 | 808 | . 4 Hunan | Mainland China | 2020-02-25T15:03:05 | 1016 | 4 | 768 | . 5 Anhui | Mainland China | 2020-02-25T06:33:02 | 989 | 6 | 712 | . 6 NaN | South Korea | 2020-02-25T08:13:19 | 977 | 10 | 22 | . 7 Jiangxi | Mainland China | 2020-02-25T15:03:05 | 934 | 1 | 683 | . 8 Shandong | Mainland China | 2020-02-25T12:53:02 | 756 | 6 | 355 | . 9 Diamond Princess cruise ship | Others | 2020-02-23T22:33:03 | 691 | 3 | 0 | . 10 Jiangsu | Mainland China | 2020-02-25T00:53:02 | 631 | 0 | 458 | . 11 Chongqing | Mainland China | 2020-02-25T23:23:03 | 576 | 6 | 372 | . 12 Sichuan | Mainland China | 2020-02-25T15:03:05 | 529 | 3 | 289 | . 13 Heilongjiang | Mainland China | 2020-02-25T15:23:04 | 480 | 12 | 243 | . 14 Beijing | Mainland China | 2020-02-25T01:33:02 | 400 | 4 | 215 | . 15 Shanghai | Mainland China | 2020-02-25T06:33:02 | 336 | 3 | 268 | . 16 NaN | Italy | 2020-02-25T18:55:32 | 322 | 10 | 1 | . 17 Hebei | Mainland China | 2020-02-25T12:43:02 | 311 | 6 | 248 | . 18 Fujian | Mainland China | 2020-02-25T10:03:07 | 294 | 1 | 199 | . 19 Guangxi | Mainland China | 2020-02-25T10:03:07 | 252 | 2 | 134 | . 20 Shaanxi | Mainland China | 2020-02-25T15:23:04 | 245 | 1 | 186 | . 21 Yunnan | Mainland China | 2020-02-25T04:53:02 | 174 | 2 | 129 | . 22 NaN | Japan | 2020-02-25T14:53:03 | 170 | 1 | 22 | . 23 Hainan | Mainland China | 2020-02-25T12:53:02 | 168 | 5 | 124 | . 24 Guizhou | Mainland China | 2020-02-25T08:03:07 | 146 | 2 | 104 | . 25 Tianjin | Mainland China | 2020-02-25T15:23:04 | 135 | 3 | 91 | . 26 Shanxi | Mainland China | 2020-02-25T23:13:03 | 133 | 0 | 98 | . 27 Liaoning | Mainland China | 2020-02-25T00:23:03 | 121 | 1 | 83 | . 28 NaN | Iran | 2020-02-25T14:53:03 | 95 | 16 | 0 | . 29 Jilin | Mainland China | 2020-02-25T15:23:04 | 93 | 1 | 63 | . ... ... | ... | ... | ... | ... | ... | . 64 San Diego County, CA | US | 2020-02-21T05:43:02 | 2 | 0 | 1 | . 65 Santa Clara, CA | US | 2020-02-21T05:23:04 | 2 | 0 | 1 | . 66 NaN | Afghanistan | 2020-02-24T23:33:02 | 1 | 0 | 0 | . 67 NaN | Algeria | 2020-02-25T23:43:03 | 1 | 0 | 0 | . 68 NaN | Belgium | 2020-02-17T04:23:06 | 1 | 0 | 1 | . 69 NaN | Cambodia | 2020-02-12T07:43:02 | 1 | 0 | 1 | . 70 London, ON | Canada | 2020-02-12T18:53:03 | 1 | 0 | 1 | . 71 NaN | Croatia | 2020-02-25T18:55:32 | 1 | 0 | 0 | . 72 NaN | Egypt | 2020-02-21T18:53:02 | 1 | 0 | 0 | . 73 NaN | Finland | 2020-02-12T00:03:12 | 1 | 0 | 1 | . 74 NaN | Iraq | 2020-02-24T23:33:02 | 1 | 0 | 0 | . 75 From Diamond Princess | Israel | 2020-02-22T20:53:02 | 1 | 0 | 0 | . 76 NaN | Lebanon | 2020-02-22T20:53:02 | 1 | 0 | 0 | . 77 Tibet | Mainland China | 2020-02-12T06:43:02 | 1 | 0 | 1 | . 78 NaN | Nepal | 2020-02-12T14:43:03 | 1 | 0 | 1 | . 79 NaN | Sri Lanka | 2020-02-08T03:43:03 | 1 | 0 | 1 | . 80 NaN | Sweden | 2020-02-01T02:13:26 | 1 | 0 | 0 | . 81 NaN | Switzerland | 2020-02-25T19:13:21 | 1 | 0 | 0 | . 82 Boston, MA | US | 2020-02-01T19:43:03 | 1 | 0 | 0 | . 83 Humboldt County, CA | US | 2020-02-21T05:13:09 | 1 | 0 | 0 | . 84 Los Angeles, CA | US | 2020-02-01T19:53:03 | 1 | 0 | 0 | . 85 Madison, WI | US | 2020-02-05T21:53:02 | 1 | 0 | 0 | . 86 Orange, CA | US | 2020-02-01T19:53:03 | 1 | 0 | 0 | . 87 Sacramento County, CA | US | 2020-02-21T23:13:16 | 1 | 0 | 0 | . 88 San Antonio, TX | US | 2020-02-13T18:53:02 | 1 | 0 | 0 | . 89 Seattle, WA | US | 2020-02-09T07:03:04 | 1 | 0 | 1 | . 90 Tempe, AZ | US | 2020-02-25T21:23:03 | 1 | 0 | 1 | . 91 Lackland, TX (From Diamond Princess) | US | 2020-02-24T23:33:02 | 0 | 0 | 0 | . 92 Omaha, NE (From Diamond Princess) | US | 2020-02-24T23:33:02 | 0 | 0 | 0 | . 93 Travis, CA (From Diamond Princess) | US | 2020-02-24T23:33:02 | 0 | 0 | 0 | . 94 rows × 6 columns . What each CSV file looks like . a.head() . Province/State Country/Region Last Update Confirmed Deaths Recovered . 0 Anhui | Mainland China | 1/23/20 17:00 | 9.0 | NaN | NaN | . 1 Beijing | Mainland China | 1/23/20 17:00 | 22.0 | NaN | NaN | . 2 Chongqing | Mainland China | 1/23/20 17:00 | 9.0 | NaN | NaN | . 3 Fujian | Mainland China | 1/23/20 17:00 | 5.0 | NaN | NaN | . 4 Gansu | Mainland China | 1/23/20 17:00 | 2.0 | NaN | NaN | . list_of_all_files = os.listdir() actual_list = [] for i in list_of_all_files: if &quot;csv&quot; in i: actual_list.append(i) . main = [] province_list = [] country_list = [] confirmed = [] death_count = [] date_list = [] for j in tqdm(actual_list): a = pd.read_csv(j) for i in range(len(a)): date_list.append(&quot;Date: &quot; + str(j[:-4])) for i in a[&#39;Province/State&#39;]: province_list.append(i) for i in a[&#39;Country/Region&#39;]: country_list.append(i) for i in a[&#39;Confirmed&#39;]: confirmed.append(i) for i in a[&#39;Deaths&#39;]: death_count.append(i) main_df = pd.DataFrame({ &#39;date&#39;: date_list, &#39;state&#39;: province_list, &#39;country&#39;:country_list, &#39;confirmed_case&#39;: confirmed, &#39;death&#39;: death_count }) . 100%|██████████| 61/61 [00:00&lt;00:00, 251.69it/s] . duplicate_dates = list(main_df[&#39;date&#39;]) . all_dates = [] for x in duplicate_dates: if x not in all_dates: all_dates.append(x) print(len(all_dates)) . 61 . list_of_ = Counter(country_list).most_common(10) . list_of_countries = [] for i in list_of_: list_of_countries.append(i[0]) print(list_of_countries) . [&#39;US&#39;, &#39;Mainland China&#39;, &#39;China&#39;, &#39;Australia&#39;, &#39;Canada&#39;, &#39;France&#39;, &#39;Japan&#39;, &#39;Thailand&#39;, &#39;Singapore&#39;, &#39;Malaysia&#39;] . sample_df = main_df[50:90] sample_df . date state country confirmed_case death . 50 Date: 02-07-2020 | NaN | UK | 3.0 | 0.0 | . 51 Date: 02-07-2020 | South Australia | Australia | 2.0 | 0.0 | . 52 Date: 02-07-2020 | Toronto, ON | Canada | 2.0 | 0.0 | . 53 Date: 02-07-2020 | NaN | Russia | 2.0 | 0.0 | . 54 Date: 02-07-2020 | Chicago, IL | US | 2.0 | 0.0 | . 55 Date: 02-07-2020 | San Benito, CA | US | 2.0 | 0.0 | . 56 Date: 02-07-2020 | Santa Clara, CA | US | 2.0 | 0.0 | . 57 Date: 02-07-2020 | NaN | Belgium | 1.0 | 0.0 | . 58 Date: 02-07-2020 | NaN | Cambodia | 1.0 | 0.0 | . 59 Date: 02-07-2020 | London, ON | Canada | 1.0 | 0.0 | . 60 Date: 02-07-2020 | NaN | Finland | 1.0 | 0.0 | . 61 Date: 02-07-2020 | Tibet | Mainland China | 1.0 | 0.0 | . 62 Date: 02-07-2020 | NaN | Nepal | 1.0 | 0.0 | . 63 Date: 02-07-2020 | NaN | Spain | 1.0 | 0.0 | . 64 Date: 02-07-2020 | NaN | Sri Lanka | 1.0 | 0.0 | . 65 Date: 02-07-2020 | NaN | Sweden | 1.0 | 0.0 | . 66 Date: 02-07-2020 | Boston, MA | US | 1.0 | 0.0 | . 67 Date: 02-07-2020 | Los Angeles, CA | US | 1.0 | 0.0 | . 68 Date: 02-07-2020 | Madison, WI | US | 1.0 | 0.0 | . 69 Date: 02-07-2020 | Orange, CA | US | 1.0 | 0.0 | . 70 Date: 02-07-2020 | Seattle, WA | US | 1.0 | 0.0 | . 71 Date: 02-07-2020 | Tempe, AZ | US | 1.0 | 0.0 | . 72 Date: 02-25-2020 | Hubei | Mainland China | 64786.0 | 2563.0 | . 73 Date: 02-25-2020 | Guangdong | Mainland China | 1347.0 | 7.0 | . 74 Date: 02-25-2020 | Henan | Mainland China | 1271.0 | 19.0 | . 75 Date: 02-25-2020 | Zhejiang | Mainland China | 1205.0 | 1.0 | . 76 Date: 02-25-2020 | Hunan | Mainland China | 1016.0 | 4.0 | . 77 Date: 02-25-2020 | Anhui | Mainland China | 989.0 | 6.0 | . 78 Date: 02-25-2020 | NaN | South Korea | 977.0 | 10.0 | . 79 Date: 02-25-2020 | Jiangxi | Mainland China | 934.0 | 1.0 | . 80 Date: 02-25-2020 | Shandong | Mainland China | 756.0 | 6.0 | . 81 Date: 02-25-2020 | Diamond Princess cruise ship | Others | 691.0 | 3.0 | . 82 Date: 02-25-2020 | Jiangsu | Mainland China | 631.0 | 0.0 | . 83 Date: 02-25-2020 | Chongqing | Mainland China | 576.0 | 6.0 | . 84 Date: 02-25-2020 | Sichuan | Mainland China | 529.0 | 3.0 | . 85 Date: 02-25-2020 | Heilongjiang | Mainland China | 480.0 | 12.0 | . 86 Date: 02-25-2020 | Beijing | Mainland China | 400.0 | 4.0 | . 87 Date: 02-25-2020 | Shanghai | Mainland China | 336.0 | 3.0 | . 88 Date: 02-25-2020 | NaN | Italy | 322.0 | 10.0 | . 89 Date: 02-25-2020 | Hebei | Mainland China | 311.0 | 6.0 | . another_dictionary = {} for i in all_dates: con = [] som = [] for index, row in sample_df.iterrows(): country = {} if i == row[&#39;date&#39;]: con.append(row[&#39;country&#39;]) som.append(row[&#39;confirmed_case&#39;]) country[&#39;countries&#39;] = con country[&#39;affected&#39;] = som another_dictionary[i] = country . list_of_countries . [&#39;US&#39;, &#39;Mainland China&#39;, &#39;China&#39;, &#39;Australia&#39;, &#39;Canada&#39;, &#39;France&#39;, &#39;Japan&#39;, &#39;Thailand&#39;, &#39;Singapore&#39;, &#39;Malaysia&#39;] . country = [] cases = [] date = [] for index, row in main_df.iterrows(): if row[&#39;country&#39;] in list_of_countries: country.append(row[&#39;country&#39;]) cases.append(row[&#39;confirmed_case&#39;]) date.append(row[&#39;date&#39;]) . Counter(country) . Counter({&#39;Mainland China&#39;: 1517, &#39;Singapore&#39;: 60, &#39;Japan&#39;: 61, &#39;Thailand&#39;: 61, &#39;Malaysia&#39;: 59, &#39;France&#39;: 127, &#39;Australia&#39;: 323, &#39;Canada&#39;: 254, &#39;UK&#39;: 40, &#39;US&#39;: 1617, &#39;China&#39;: 396}) . print(len(date)) . 4515 . new = pd.DataFrame() new[&quot;date&quot;] = date new[&quot;country&quot;] = country new[&quot;confirmed_case&quot;] = cases . new.head() . date country confirmed_case . 0 Date: 02-07-2020 | Mainland China | 24953.0 | . 1 Date: 02-07-2020 | Mainland China | 1034.0 | . 2 Date: 02-07-2020 | Mainland China | 1006.0 | . 3 Date: 02-07-2020 | Mainland China | 914.0 | . 4 Date: 02-07-2020 | Mainland China | 772.0 | . # another_dictionary[&#39;Date: 02-07-2020&#39;] . outer = {} for i in another_dictionary: countries = [] case_count = [] newest = {} for j, k in zip(another_dictionary[i][&#39;countries&#39;], another_dictionary[i][&#39;affected&#39;]): # print(j, k) if j not in countries: countries.append(j) case_count.append(k) else: case_no = countries.index(j) case_count[case_no] += k for l,m in zip(countries, case_count): newest[l] = m outer[i] = newest # print(outer) let = pd.DataFrame() countries = list_of_countries cases = [] for key, value in outer.items(): for inner_key, inner_value in value.items(): print(&quot;country: &quot;, inner_key, &quot; case type: &quot;, type(inner_value)) if inner_key not in countries: countries.append(inner_key) even_inner = [] even_inner.append(inner_value) cases.append(even_inner) else: case_no = countries.index(j) cases[case_no].append(int(inner_value)) total = 0 for i in cases: if len(i) &gt; total: total = len(i) print(total) new_cases = [] for i in cases: if len(i) == 0: new = [0.0] * total new_cases.append(new) else: new = i + [i[-1]] * (total - len(i)) new_cases.append(new) . country: UK case type: &lt;class &#39;float&#39;&gt; country: Australia case type: &lt;class &#39;float&#39;&gt; . IndexError Traceback (most recent call last) &lt;ipython-input-210-d6f8051713b3&gt; in &lt;module&gt; 31 else: 32 case_no = countries.index(j) &gt; 33 cases[case_no].append(int(inner_value)) 34 35 total = 0 IndexError: list index out of range . let[&quot;Date&quot;] = date_list for i,j in zip(countries,new_cases): let[i] = j # let[&quot;Confirmed&quot;] = new_cases . let . Date UK Australia Canada Russia US Belgium Cambodia Finland Mainland China Nepal Spain Sri Lanka Sweden South Korea Others Italy . 0 Date: 02-07-2020 | 3.0 | 2.0 | 3.0 | 2.0 | 12.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 977.0 | 691.0 | 322.0 | . 1 Date: 02-25-2020 | 3.0 | 2.0 | 3.0 | 2.0 | 12.0 | 1.0 | 1.0 | 1.0 | 75567.0 | 1.0 | 1.0 | 1.0 | 1.0 | 977.0 | 691.0 | 322.0 | . print(len(countries)) print(cases) . 16 [[3.0], [2.0], [3.0], [2.0], [12.0], [1.0], [1.0], [1.0], [1.0, 75567], [1.0], [1.0], [1.0], [1.0], [977.0], [691.0], [322.0]] . u = [0.0,2.0, 3.4] u.append(0.0) print(u) . [0.0, 2.0, 3.4, 0.0] . duplicate_dates = list(main_df[&#39;date&#39;]) all_dates = [] for x in duplicate_dates: if x not in all_dates: all_dates.append(x) len(all_dates) . 61 . def checking(df): duplicate_dates = list(df[&#39;date&#39;]) all_dates = [] for x in duplicate_dates: if x not in all_dates: all_dates.append(x) another_dictionary = {} for i in all_dates: con = [] som = [] for index, row in df.iterrows(): country = {} if i == row[&#39;date&#39;]: con.append(row[&#39;country&#39;]) som.append(row[&#39;confirmed_case&#39;]) country[&#39;countries&#39;] = con country[&#39;affected&#39;] = som another_dictionary[i] = country outer = {} for i in another_dictionary: countries = [] case_count = [] newest = {} for j, k in zip(another_dictionary[i][&#39;countries&#39;], another_dictionary[i][&#39;affected&#39;]): if j not in countries: countries.append(j) case_count.append(k) else: case_no = countries.index(j) case_count[case_no] += k for l,m in zip(countries, case_count): newest[l] = m outer[i] = newest let = pd.DataFrame() countries = [] cases = [] for key, value in outer.items(): for inner_key, inner_value in value.items(): # print(&quot;country: &quot;, inner_key, &quot; case type: &quot;, type(inner_value)) if inner_key not in countries: countries.append(inner_key) even_inner = [] even_inner.append(inner_value) cases.append(even_inner) else: if inner_value.is_integer(): case_no = countries.index(j) cases[case_no].append(int(inner_value)) else: case_no = countries.index(j) cases[case_no].append(0) total = 0 for i in cases: if len(i) &gt; total: total = len(i) # print(total) new_cases = [] for i in cases: if len(i) == 0: new = [0.0] * total new_cases.append(new) else: new = i + [i[-1]] * (total - len(i)) new_cases.append(new) # let[&quot;Date&quot;] = all_dates for i,j in zip(countries,new_cases): let[i] = j return let . sam = checking(new) . sam . Mainland China Singapore Japan Thailand Malaysia France Australia Canada UK US China . 0 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 6.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 1 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 77660.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 2 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 170.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 3 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 91.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 4 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 37.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 5 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 53.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 6 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 22.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 7 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 14.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 8 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 13.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 9 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 22.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 10 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 11.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 11 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 0.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 12 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 1.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 13 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 2.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 14 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 2.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 15 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 0.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 16 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 1.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 17 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 1.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 18 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 3.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 19 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 1.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 20 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 0.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 21 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 0.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 22 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 0.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 23 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 2.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 24 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 2.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 25 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 5.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 26 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 3.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 27 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 2.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 28 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 1399.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 29 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 2.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 540 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 681.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 541 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 272.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 542 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 800.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 543 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 81250.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 544 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 12632.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 545 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 19101.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 546 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 1030.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 547 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 963.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 548 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 385.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 549 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 791.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 550 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 322.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 551 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 943.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 552 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 81305.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 553 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 14308.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 554 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 25493.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 555 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 1183.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 556 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 1007.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 557 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 1071.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 558 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 432.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 559 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 1278.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 560 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 411.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 561 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 81397.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 562 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 16044.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 563 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 33276.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 564 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 1306.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 565 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 1086.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 566 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 599.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 567 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 1314.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 568 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 455.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 569 34075.0 | 30.0 | 25.0 | 25.0 | 12.0 | 1465.0 | 15.0 | 7.0 | 3.0 | 12.0 | 80921.0 | . 570 rows × 11 columns . newest = {} for key, value in another_dictionary[&#39;Date: 02-07-2020&#39;].items(): if key == &quot;countries&quot;: countries = [] for x in value: if x not in countries: countries.append(x) newest[&#39;countries&#39;] = countries elif key == &quot;affected&quot;: . ues . # new_df = pd.DataFrame() a_dictionary = {} for i in list_of_countries: case_of_confirmed = [] date_list = [] for index, row in tqdm(sample_df.iterrows()): if i == row[&#39;country&#39;]: for k in all_dates: if k == row[&#39;date&#39;]: print(i) print(k) print(row[&#39;confirmed_case&#39;]) case_of_confirmed.append(row[&#39;confirmed_case&#39;]) # date_list.append(row[&#39;date&#39;]) a_dictionary[i] = case_of_confirmed . 30it [00:00, 1838.61it/s] 30it [00:00, 452.25it/s] 30it [00:00, 584.35it/s] . US Date: 02-07-2020 2.0 US Date: 02-07-2020 2.0 US Date: 02-07-2020 2.0 Mainland China Date: 02-07-2020 18.0 . 30it [00:00, 211.53it/s] 0it [00:00, ?it/s] . Australia Date: 02-07-2020 5.0 Australia Date: 02-07-2020 4.0 Australia Date: 02-07-2020 4.0 Australia Date: 02-07-2020 2.0 Canada Date: 02-07-2020 4.0 Canada Date: 02-07-2020 2.0 . 30it [00:00, 289.66it/s] 30it [00:00, 398.40it/s] 0it [00:00, ?it/s] . Canada Date: 02-07-2020 1.0 France Date: 02-07-2020 6.0 Japan Date: 02-07-2020 25.0 . 30it [00:00, 232.61it/s] 30it [00:00, 382.21it/s] 30it [00:00, 414.20it/s] 0it [00:00, ?it/s] . Thailand Date: 02-07-2020 25.0 Singapore Date: 02-07-2020 30.0 . 30it [00:00, 259.57it/s] . Malaysia Date: 02-07-2020 12.0 . . for key, value in a_dictionary.items(): print(value) # total = 0 # for i in value: # if i.is_integer(): # total += int(i) # a_dictionary[key] = total . [2.0, 2.0, 2.0] [18.0] [] [5.0, 4.0, 4.0, 2.0] [4.0, 2.0, 1.0] [6.0] [25.0] [25.0] [30.0] [12.0] . a_dictionary . {&#39;US&#39;: 6, &#39;Mainland China&#39;: 18, &#39;China&#39;: 0, &#39;Australia&#39;: 15, &#39;Canada&#39;: 7, &#39;France&#39;: 6, &#39;Japan&#39;: 25, &#39;Thailand&#39;: 25, &#39;Singapore&#39;: 30, &#39;Malaysia&#39;: 12} . # list_of_all_files . actual_list = [] for i in list_of_all_files: if &quot;csv&quot; in i: actual_list.append(i) . main_df = pd.DataFrame({ &#39;date&#39;: date_list, &#39;state&#39;: province_list, &#39;country&#39;:country_list, &#39;confirmed_case&#39;: confirmed, &#39;death&#39;: death_count }) . len(main_df) . 7926 . Counter(country_list) . Counter({&#39;Mainland China&#39;: 1517, &#39;Others&#39;: 33, &#39;Singapore&#39;: 60, &#39;Hong Kong&#39;: 48, &#39;Japan&#39;: 61, &#39;Thailand&#39;: 61, &#39;South Korea&#39;: 48, &#39;Taiwan&#39;: 48, &#39;Germany&#39;: 55, &#39;Malaysia&#39;: 59, &#39;Macau&#39;: 48, &#39;Vietnam&#39;: 59, &#39;France&#39;: 127, &#39;Australia&#39;: 323, &#39;United Arab Emirates&#39;: 54, &#39;Canada&#39;: 254, &#39;India&#39;: 53, &#39;Italy&#39;: 52, &#39;Philippines&#39;: 54, &#39;UK&#39;: 40, &#39;Russia&#39;: 51, &#39;US&#39;: 1617, &#39;Belgium&#39;: 48, &#39;Cambodia&#39;: 56, &#39;Finland&#39;: 54, &#39;Nepal&#39;: 58, &#39;Spain&#39;: 51, &#39;Sri Lanka&#39;: 56, &#39;Sweden&#39;: 52, &#39;Iran&#39;: 32, &#39;Bahrain&#39;: 28, &#39;Kuwait&#39;: 28, &#39;Austria&#39;: 27, &#39;Oman&#39;: 28, &#39;Afghanistan&#39;: 28, &#39;Algeria&#39;: 27, &#39;Croatia&#39;: 27, &#39;Egypt&#39;: 38, &#39;Iraq&#39;: 29, &#39;Israel&#39;: 31, &#39;Lebanon&#39;: 31, &#39;Switzerland&#39;: 27, &#39;Mexico&#39;: 25, &#39;Brazil&#39;: 27, &#39;Colombia&#39;: 18, &#39;Ivory Coast&#39;: 1, &#39;Pakistan&#39;: 26, &#39;Georgia&#39;: 26, &#39;Greece&#39;: 26, &#39;North Macedonia&#39;: 26, &#39;Norway&#39;: 26, &#39;Romania&#39;: 26, &#39;Denmark&#39;: 41, &#39;Estonia&#39;: 25, &#39;Netherlands&#39;: 40, &#39;San Marino&#39;: 25, &#39; Azerbaijan&#39;: 1, &#39;Belarus&#39;: 24, &#39;Iceland&#39;: 24, &#39;Lithuania&#39;: 24, &#39;New Zealand&#39;: 24, &#39;Nigeria&#39;: 24, &#39;North Ireland&#39;: 1, &#39;Ireland&#39;: 23, &#39;Luxembourg&#39;: 23, &#39;Monaco&#39;: 23, &#39;Qatar&#39;: 23, &#39;Ecuador&#39;: 22, &#39;Azerbaijan&#39;: 22, &#39;Czech Republic&#39;: 10, &#39;Armenia&#39;: 22, &#39;Dominican Republic&#39;: 22, &#39;Indonesia&#39;: 21, &#39;Portugal&#39;: 21, &#39;Andorra&#39;: 21, &#39;Latvia&#39;: 21, &#39;Morocco&#39;: 21, &#39;Saudi Arabia&#39;: 21, &#39;Senegal&#39;: 21, &#39;Argentina&#39;: 20, &#39;Chile&#39;: 20, &#39;Jordan&#39;: 20, &#39;Ukraine&#39;: 20, &#39;Saint Barthelemy&#39;: 7, &#39;Hungary&#39;: 19, &#39;Faroe Islands&#39;: 7, &#39;Gibraltar&#39;: 7, &#39;Liechtenstein&#39;: 19, &#39;Poland&#39;: 19, &#39;Tunisia&#39;: 19, &#39;Palestine&#39;: 5, &#39;Bosnia and Herzegovina&#39;: 18, &#39;Slovenia&#39;: 18, &#39;South Africa&#39;: 18, &#39;Bhutan&#39;: 17, &#39;Cameroon&#39;: 17, &#39;Costa Rica&#39;: 17, &#39;Peru&#39;: 17, &#39;Serbia&#39;: 17, &#39;Slovakia&#39;: 17, &#39;Togo&#39;: 17, &#39;Vatican City&#39;: 4, &#39;French Guiana&#39;: 15, &#39;Malta&#39;: 16, &#39;Martinique&#39;: 16, &#39;Republic of Ireland&#39;: 1, &#39;Bulgaria&#39;: 15, &#39;Maldives&#39;: 15, &#39;Bangladesh&#39;: 15, &#39;Moldova&#39;: 14, &#39;Paraguay&#39;: 15, &#39;Albania&#39;: 14, &#39;Cyprus&#39;: 14, &#39;St. Martin&#39;: 1, &#39;Brunei&#39;: 14, &#39;Iran (Islamic Republic of)&#39;: 1, &#39;Republic of Korea&#39;: 1, &#39;Hong Kong SAR&#39;: 1, &#39;Taipei and environs&#39;: 1, &#39;Viet Nam&#39;: 1, &#39;occupied Palestinian territory&#39;: 7, &#39;Macao SAR&#39;: 1, &#39;Russian Federation&#39;: 1, &#39;Republic of Moldova&#39;: 1, &#39;Saint Martin&#39;: 1, &#39;Burkina Faso&#39;: 13, &#39;Channel Islands&#39;: 1, &#39;Holy See&#39;: 13, &#39;Mongolia&#39;: 13, &#39;Panama&#39;: 13, &#39;China&#39;: 396, &#39;Korea, South&#39;: 12, &#39;Cruise Ship&#39;: 12, &#39;United Kingdom&#39;: 55, &#39;Czechia&#39;: 12, &#39;Taiwan*&#39;: 12, &#39;Bolivia&#39;: 12, &#39;Honduras&#39;: 12, &#39;Congo (Kinshasa)&#39;: 12, &#34;Cote d&#39;Ivoire&#34;: 12, &#39;Jamaica&#39;: 12, &#39;Reunion&#39;: 12, &#39;Turkey&#39;: 12, &#39;Cuba&#39;: 11, &#39;Guyana&#39;: 11, &#39;Kazakhstan&#39;: 10, &#39;Cayman Islands&#39;: 3, &#39;Guadeloupe&#39;: 10, &#39;Ethiopia&#39;: 10, &#39;Sudan&#39;: 10, &#39;Guinea&#39;: 10, &#39;Antigua and Barbuda&#39;: 10, &#39;Aruba&#39;: 7, &#39;Kenya&#39;: 10, &#39;Uruguay&#39;: 9, &#39;Ghana&#39;: 9, &#39;Jersey&#39;: 9, &#39;Namibia&#39;: 9, &#39;Seychelles&#39;: 9, &#39;Trinidad and Tobago&#39;: 9, &#39;Venezuela&#39;: 9, &#39;Curacao&#39;: 2, &#39;Eswatini&#39;: 9, &#39;Gabon&#39;: 9, &#39;Guatemala&#39;: 9, &#39;Guernsey&#39;: 9, &#39;Mauritania&#39;: 9, &#39;Rwanda&#39;: 9, &#39;Saint Lucia&#39;: 9, &#39;Saint Vincent and the Grenadines&#39;: 9, &#39;Suriname&#39;: 9, &#39;Kosovo&#39;: 8, &#39;Central African Republic&#39;: 8, &#39;Congo (Brazzaville)&#39;: 8, &#39;Equatorial Guinea&#39;: 8, &#39;Uzbekistan&#39;: 8, &#39;Guam&#39;: 7, &#39;Puerto Rico&#39;: 7, &#39;Benin&#39;: 7, &#39;Greenland&#39;: 7, &#39;Liberia&#39;: 7, &#39;Mayotte&#39;: 7, &#39;Republic of the Congo&#39;: 7, &#39;Somalia&#39;: 7, &#39;Tanzania&#39;: 7, &#39;The Bahamas&#39;: 7, &#39;Barbados&#39;: 6, &#39;Montenegro&#39;: 6, &#39;The Gambia&#39;: 6, &#39;Kyrgyzstan&#39;: 5, &#39;Mauritius&#39;: 5, &#39;Zambia&#39;: 5, &#39;Djibouti&#39;: 5, &#39;Gambia, The&#39;: 5, &#39;Bahamas, The&#39;: 4, &#39;Chad&#39;: 4, &#39;El Salvador&#39;: 4, &#39;Fiji&#39;: 4, &#39;Nicaragua&#39;: 4, &#39;Madagascar&#39;: 3, &#39;Haiti&#39;: 3, &#39;Angola&#39;: 3, &#39;Cabo Verde&#39;: 3, &#39;Niger&#39;: 3, &#39;Papua New Guinea&#39;: 3, &#39;Zimbabwe&#39;: 3, &#39;Cape Verde&#39;: 2, &#39;East Timor&#39;: 2, &#39;Eritrea&#39;: 2, &#39;Uganda&#39;: 2, &#39;Dominica&#39;: 1, &#39;Grenada&#39;: 1, &#39;Mozambique&#39;: 1, &#39;Syria&#39;: 1, &#39;Timor-Leste&#39;: 1}) . Counter(country_list).most_common(10) . [(&#39;US&#39;, 1617), (&#39;Mainland China&#39;, 1517), (&#39;China&#39;, 396), (&#39;Australia&#39;, 323), (&#39;Canada&#39;, 254), (&#39;France&#39;, 127), (&#39;Japan&#39;, 61), (&#39;Thailand&#39;, 61), (&#39;Singapore&#39;, 60), (&#39;Malaysia&#39;, 59)] . def specific_country(search): # total_confirmed_case = 0.0 case_of_confirmed = [] case_of_death = [] date = [] state = [] for index, row in main_df.iterrows(): if row[&#39;country&#39;] == search: case_of_confirmed.append(row[&#39;confirmed_case&#39;]) case_of_death.append(row[&#39;death&#39;]) date.append(row[&#39;date&#39;]) state.append(row[&#39;state&#39;]) # print(total_confirmed_case) bd_df = pd.DataFrame({ &#39;date&#39;: date, &#39;confirmed_case&#39;: case_of_confirmed, &#39;death&#39;: case_of_death, &#39;state&#39;: state }) return bd_df . bd_df = specific_country(&quot;Bangladesh&quot;) . ax = plt.figure(figsize=(20,10)) # Add title ax = plt.title(&quot;Confirmed Cases and Death Count due to COVID-19 in Bangladesh&quot;) ax = sns.lineplot(y=bd_df[&#39;confirmed_case&#39;], x=bd_df[&#39;date&#39;], color = &#39;Blue&#39;, label = &quot;Confirmed Case&quot;) ax = sns.lineplot(x = bd_df[&#39;date&#39;], y = bd_df[&#39;death&#39;], color = &#39;red&#39;, label = &quot;Death Count&quot;) ax.legend() # ax = sns.color_palette(&quot;RdBu&quot;, n_colors=7) ax.set_xticklabels(labels=bd_df[&#39;date&#39;], rotation=45, ha=&#39;right&#39;) ax = plt.ylabel(&quot;Count&quot;) . plt.figure(figsize=(14,6)) # Add title plt.title(&quot;Rise of COVID-19 in Bangladesh&quot;) sns.lineplot(data=bd_df[&#39;confirmed_case&#39;], label=&quot;Confirmed Case&quot;) sns.lineplot(data=bd_df[&#39;death&#39;], label=&quot;Death Toll&quot;, color = &quot;red&quot;) # Add label for horizontal axis plt.xlabel(&quot;Date&quot;) . Text(0.5, 0, &#39;Date&#39;) . us_df = specific_country(&quot;US&quot;) us_df.head() . date confirmed_case death state . 0 Date: 02-07-2020 | 2.0 | 0.0 | Chicago, IL | . 1 Date: 02-07-2020 | 2.0 | 0.0 | San Benito, CA | . 2 Date: 02-07-2020 | 2.0 | 0.0 | Santa Clara, CA | . 3 Date: 02-07-2020 | 1.0 | 0.0 | Boston, MA | . 4 Date: 02-07-2020 | 1.0 | 0.0 | Los Angeles, CA | . Counter(list(us_df[&#39;state&#39;])) . Counter({&#39;Chicago, IL&#39;: 30, &#39;San Benito, CA&#39;: 36, &#39;Santa Clara, CA&#39;: 35, &#39;Boston, MA&#39;: 34, &#39;Los Angeles, CA&#39;: 38, &#39;Madison, WI&#39;: 34, &#39;Orange, CA&#39;: 32, &#39;Seattle, WA&#39;: 30, &#39;Tempe, AZ&#39;: 35, &#39;Unassigned Location (From Diamond Princess)&#39;: 15, &#39;San Diego County, CA&#39;: 28, &#39;Humboldt County, CA&#39;: 18, &#39;Sacramento County, CA&#39;: 18, &#39;San Antonio, TX&#39;: 26, &#39;Lackland, TX (From Diamond Princess)&#39;: 17, &#39;Omaha, NE (From Diamond Princess)&#39;: 17, &#39;Travis, CA (From Diamond Princess)&#39;: 17, &#39;Washington&#39;: 23, &#39;Chicago&#39;: 1, &#39;Illinois&#39;: 20, &#39;California&#39;: 19, &#39;Arizona&#39;: 19, &#39;Ashland, NE&#39;: 1, &#39;Travis, CA&#39;: 1, &#39;Lackland, TX&#39;: 1, &#39;Portland, OR&#39;: 3, &#39;Snohomish County, WA&#39;: 10, &#39;Providence, RI&#39;: 6, &#39;King County, WA&#39;: 8, &#39;Cook County, IL&#39;: 8, &#39;Grafton County, NH&#39;: 8, &#39;Hillsborough, FL&#39;: 8, &#39;New York City, NY&#39;: 4, &#39;Placer County, CA&#39;: 8, &#39;San Mateo, CA&#39;: 8, &#39;Sarasota, FL&#39;: 8, &#39;Sonoma County, CA&#39;: 8, &#39;Umatilla, OR&#39;: 8, &#39;Fulton County, GA&#39;: 7, &#39;Washington County, OR&#39;: 7, &#39; Norfolk County, MA&#39;: 5, &#39;Berkeley, CA&#39;: 4, &#39;Maricopa County, AZ&#39;: 7, &#39;Wake County, NC&#39;: 7, &#39;Westchester County, NY&#39;: 7, &#39;Orange County, CA&#39;: 6, &#39;Contra Costa County, CA&#39;: 6, &#39;Bergen County, NJ&#39;: 5, &#39;Harris County, TX&#39;: 5, &#39;San Francisco County, CA&#39;: 5, &#39;Clark County, NV&#39;: 5, &#39;Fort Bend County, TX&#39;: 5, &#39;Grant County, WA&#39;: 5, &#39;Queens County, NY&#39;: 1, &#39;Santa Rosa County, FL&#39;: 5, &#39;Williamson County, TN&#39;: 5, &#39;New York County, NY&#39;: 4, &#39;Unassigned Location, WA&#39;: 4, &#39;Montgomery County, MD&#39;: 4, &#39;Suffolk County, MA&#39;: 4, &#39;Denver County, CO&#39;: 4, &#39;Summit County, CO&#39;: 4, &#39;Chatham County, NC&#39;: 4, &#39;Delaware County, PA&#39;: 4, &#39;Douglas County, NE&#39;: 4, &#39;Fayette County, KY&#39;: 4, &#39;Floyd County, GA&#39;: 2, &#39;Marion County, IN&#39;: 4, &#39;Middlesex County, MA&#39;: 4, &#39;Nassau County, NY&#39;: 4, &#39;Norwell County, MA&#39;: 1, &#39;Ramsey County, MN&#39;: 4, &#39;Washoe County, NV&#39;: 4, &#39;Wayne County, PA&#39;: 4, &#39;Yolo County, CA&#39;: 4, &#39;Santa Clara County, CA&#39;: 3, &#39;Grand Princess Cruise Ship&#39;: 3, &#39;Douglas County, CO&#39;: 3, &#39;Providence County, RI&#39;: 3, &#39;Alameda County, CA&#39;: 3, &#39;Broward County, FL&#39;: 3, &#39;Fairfield County, CT&#39;: 3, &#39;Lee County, FL&#39;: 3, &#39;Pinal County, AZ&#39;: 3, &#39;Rockland County, NY&#39;: 3, &#39;Saratoga County, NY&#39;: 3, &#39;Charleston County, SC&#39;: 3, &#39;Clark County, WA&#39;: 3, &#39;Cobb County, GA&#39;: 3, &#39;Davis County, UT&#39;: 3, &#39;El Paso County, CO&#39;: 3, &#39;Honolulu County, HI&#39;: 3, &#39;Jackson County, OR &#39;: 3, &#39;Jefferson County, WA&#39;: 3, &#39;Kershaw County, SC&#39;: 3, &#39;Klamath County, OR&#39;: 3, &#39;Madera County, CA&#39;: 3, &#39;Pierce County, WA&#39;: 3, &#39;Plymouth County, MA&#39;: 3, &#39;Santa Cruz County, CA&#39;: 1, &#39;Tulsa County, OK&#39;: 3, &#39;Montgomery County, TX&#39;: 3, &#39;Norfolk County, MA&#39;: 2, &#39;Montgomery County, PA&#39;: 2, &#39;Fairfax County, VA&#39;: 2, &#39;Rockingham County, NH&#39;: 2, &#39;Washington, D.C.&#39;: 2, &#39;Berkshire County, MA&#39;: 2, &#39;Davidson County, TN&#39;: 2, &#39;Douglas County, OR&#39;: 2, &#39;Fresno County, CA&#39;: 2, &#39;Harford County, MD&#39;: 2, &#39;Hendricks County, IN&#39;: 2, &#39;Hudson County, NJ&#39;: 2, &#39;Johnson County, KS&#39;: 2, &#39;Kittitas County, WA&#39;: 2, &#39;Manatee County, FL&#39;: 2, &#39;Marion County, OR&#39;: 2, &#39;Okaloosa County, FL&#39;: 2, &#39;Polk County, GA&#39;: 2, &#39;Riverside County, CA&#39;: 2, &#39;Shelby County, TN&#39;: 2, &#39;Spokane County, WA&#39;: 2, &#39;St. Louis County, MO&#39;: 2, &#39;Suffolk County, NY&#39;: 2, &#39;Ulster County, NY&#39;: 2, &#39;Unassigned Location, VT&#39;: 1, &#39;Unknown Location, MA&#39;: 2, &#39;Volusia County, FL&#39;: 2, &#39;Johnson County, IA&#39;: 1, &#39;Harrison County, KY&#39;: 1, &#39;Bennington County, VT&#39;: 1, &#39;Carver County, MN&#39;: 1, &#39;Charlotte County, FL&#39;: 1, &#39;Cherokee County, GA&#39;: 1, &#39;Collin County, TX&#39;: 1, &#39;Jefferson County, KY&#39;: 1, &#39;Jefferson Parish, LA&#39;: 1, &#39;Shasta County, CA&#39;: 1, &#39;Spartanburg County, SC&#39;: 1, &#39;New York&#39;: 13, &#39;Massachusetts&#39;: 13, &#39;Diamond Princess&#39;: 13, &#39;Grand Princess&#39;: 13, &#39;Georgia&#39;: 13, &#39;Colorado&#39;: 13, &#39;Florida&#39;: 13, &#39;New Jersey&#39;: 13, &#39;Oregon&#39;: 13, &#39;Texas&#39;: 13, &#39;Pennsylvania&#39;: 13, &#39;Iowa&#39;: 13, &#39;Maryland&#39;: 13, &#39;North Carolina&#39;: 13, &#39;South Carolina&#39;: 13, &#39;Tennessee&#39;: 13, &#39;Virginia&#39;: 13, &#39;Indiana&#39;: 13, &#39;Kentucky&#39;: 13, &#39;District of Columbia&#39;: 13, &#39;Nevada&#39;: 13, &#39;New Hampshire&#39;: 13, &#39;Minnesota&#39;: 13, &#39;Nebraska&#39;: 13, &#39;Ohio&#39;: 13, &#39;Rhode Island&#39;: 13, &#39;Wisconsin&#39;: 13, &#39;Connecticut&#39;: 13, &#39;Hawaii&#39;: 13, &#39;Oklahoma&#39;: 13, &#39;Utah&#39;: 13, &#39;Kansas&#39;: 13, &#39;Louisiana&#39;: 13, &#39;Missouri&#39;: 13, &#39;Vermont&#39;: 13, &#39;Alaska&#39;: 12, &#39;Arkansas&#39;: 13, &#39;Delaware&#39;: 13, &#39;Idaho&#39;: 13, &#39;Maine&#39;: 13, &#39;Michigan&#39;: 13, &#39;Mississippi&#39;: 13, &#39;Montana&#39;: 13, &#39;New Mexico&#39;: 13, &#39;North Dakota&#39;: 13, &#39;South Dakota&#39;: 13, &#39;West Virginia&#39;: 13, &#39;Wyoming&#39;: 13, &#39;Alabama&#39;: 10, &#39;Puerto Rico&#39;: 9, &#39;Virgin Islands, U.S.&#39;: 2, &#39;Guam&#39;: 8, &#39;Virgin Islands&#39;: 4, &#39;United States Virgin Islands&#39;: 5, &#39;US&#39;: 5}) . duplicates_included = list(us_df[&#39;state&#39;]) . duplicate_dates = list(us_df[&#39;date&#39;]) . h = duplicate_dates[6] . if h not in duplicate_dates: pass else: print(&quot;no&quot;) . no . all_dates = [] for x in duplicate_dates: if x not in all_dates: all_dates.append(x) . len(all_dates) . 61 . all_states = [] for x in duplicates_included: if x not in all_states: all_states.append(x) . new_df = pd.DataFrame() a_dictionary = {} for i in all_states: case_of_confirmed = [] date_list = [] for index, row in tqdm(us_df.iterrows()): if i == row[&#39;state&#39;]: case_of_confirmed.append(row[&#39;confirmed_case&#39;]) date_list.append(row[&#39;date&#39;]) a_dictionary[i] = case_of_confirmed . 1617it [00:00, 9897.03it/s] 1617it [00:00, 11750.16it/s] 1617it [00:00, 11742.24it/s] 1617it [00:00, 11434.92it/s] 1617it [00:00, 11893.80it/s] 1617it [00:00, 10751.23it/s] 1617it [00:00, 10138.73it/s] 1617it [00:00, 9356.25it/s] 1617it [00:00, 10347.46it/s] 1617it [00:00, 11522.09it/s] 1617it [00:00, 10398.69it/s] 1617it [00:00, 10219.87it/s] 1617it [00:00, 9235.74it/s] 1617it [00:00, 9264.53it/s] 1617it [00:00, 9003.03it/s] 1617it [00:00, 9227.36it/s] 1617it [00:00, 9227.12it/s] 1617it [00:00, 9034.51it/s] 1617it [00:00, 8195.64it/s] 1617it [00:00, 8527.91it/s] 1617it [00:00, 8160.34it/s] 1617it [00:00, 8660.94it/s] 1617it [00:00, 8609.87it/s] 1617it [00:00, 7859.78it/s] 1617it [00:00, 8486.59it/s] 1617it [00:00, 9129.04it/s] 1617it [00:00, 9170.49it/s] 1617it [00:00, 8998.69it/s] 1617it [00:00, 8724.91it/s] 1617it [00:00, 9067.24it/s] 1617it [00:00, 8660.79it/s] 1617it [00:00, 9139.96it/s] 1617it [00:00, 9242.53it/s] 1617it [00:00, 8815.31it/s] 1617it [00:00, 8323.04it/s] 1617it [00:00, 9277.44it/s] 1617it [00:00, 9151.53it/s] 1617it [00:00, 9443.72it/s] 1617it [00:00, 9101.01it/s] 1617it [00:00, 9480.81it/s] 1617it [00:00, 8805.89it/s] 1617it [00:00, 8654.21it/s] 1617it [00:00, 8923.72it/s] 1617it [00:00, 6763.55it/s] 1617it [00:00, 9370.28it/s] 1617it [00:00, 9316.31it/s] 1617it [00:00, 9550.51it/s] 1617it [00:00, 9002.05it/s] 1617it [00:00, 9469.38it/s] 1617it [00:00, 9253.00it/s] 1617it [00:00, 8832.69it/s] 1617it [00:00, 8076.19it/s] 1617it [00:00, 9120.41it/s] 1617it [00:00, 9088.79it/s] 1617it [00:00, 8280.46it/s] 1617it [00:00, 8369.44it/s] 1617it [00:00, 7817.37it/s] 1617it [00:00, 9069.88it/s] 1617it [00:00, 9476.14it/s] 1617it [00:00, 9548.61it/s] 1617it [00:00, 8863.21it/s] 1617it [00:00, 7681.98it/s] 1617it [00:00, 8134.70it/s] 1617it [00:00, 8906.50it/s] 1617it [00:00, 9616.28it/s] 1617it [00:00, 9422.23it/s] 1617it [00:00, 8871.82it/s] 1617it [00:00, 9173.40it/s] 1617it [00:00, 9087.24it/s] 1617it [00:00, 9593.06it/s] 1617it [00:00, 9458.01it/s] 1617it [00:00, 9704.04it/s] 1617it [00:00, 8506.53it/s] 1617it [00:00, 8809.24it/s] 1617it [00:00, 9324.11it/s] 1617it [00:00, 9138.52it/s] 1617it [00:00, 9295.73it/s] 1617it [00:00, 9470.56it/s] 1617it [00:00, 8840.73it/s] 1617it [00:00, 9301.09it/s] 1617it [00:00, 9254.12it/s] 1617it [00:00, 9076.39it/s] 1617it [00:00, 9507.97it/s] 1617it [00:00, 8809.27it/s] 1617it [00:00, 9387.24it/s] 1617it [00:00, 9530.12it/s] 1617it [00:00, 8957.30it/s] 1617it [00:00, 9329.12it/s] 1617it [00:00, 9365.45it/s] 1617it [00:00, 8436.51it/s] 1617it [00:00, 9220.87it/s] 1617it [00:00, 9055.25it/s] 1617it [00:00, 9792.71it/s] 1617it [00:00, 8957.36it/s] 1617it [00:00, 9159.69it/s] 1617it [00:00, 8534.27it/s] 1617it [00:00, 9510.67it/s] 1617it [00:00, 9482.52it/s] 1617it [00:00, 9440.95it/s] 1617it [00:00, 9413.64it/s] 1617it [00:00, 8774.43it/s] 1617it [00:00, 8972.99it/s] 1617it [00:00, 9310.22it/s] 1617it [00:00, 9486.76it/s] 1617it [00:00, 9711.50it/s] 1617it [00:00, 9616.58it/s] 1617it [00:00, 8461.64it/s] 1617it [00:00, 9290.35it/s] 1617it [00:00, 9442.05it/s] 1617it [00:00, 9490.33it/s] 1617it [00:00, 8717.96it/s] 1617it [00:00, 8403.14it/s] 1617it [00:00, 7958.63it/s] 1617it [00:00, 8583.46it/s] 1617it [00:00, 8710.26it/s] 1617it [00:00, 8792.19it/s] 1617it [00:00, 9056.12it/s] 1617it [00:00, 8133.16it/s] 1617it [00:00, 9167.86it/s] 1617it [00:00, 8401.98it/s] 1617it [00:00, 8888.65it/s] 1617it [00:00, 9304.73it/s] 1617it [00:00, 8224.59it/s] 1617it [00:00, 9238.79it/s] 1617it [00:00, 8862.06it/s] 1617it [00:00, 9083.52it/s] 1617it [00:00, 8734.28it/s] 1617it [00:00, 9274.50it/s] 1617it [00:00, 9002.36it/s] 1617it [00:00, 9638.61it/s] 1617it [00:00, 9430.83it/s] 1617it [00:00, 9472.25it/s] 1617it [00:00, 9332.68it/s] 1617it [00:00, 8101.39it/s] 1617it [00:00, 7268.92it/s] 1617it [00:00, 9203.90it/s] 1617it [00:00, 9422.46it/s] 1617it [00:00, 9346.25it/s] 1617it [00:00, 9427.41it/s] 1617it [00:00, 8463.77it/s] 1617it [00:00, 9705.91it/s] 1617it [00:00, 9152.91it/s] 1617it [00:00, 9367.32it/s] 1617it [00:00, 9272.37it/s] 1617it [00:00, 8533.95it/s] 1617it [00:00, 8920.42it/s] 1617it [00:00, 8939.04it/s] 1617it [00:00, 8863.05it/s] 1617it [00:00, 8606.84it/s] 1617it [00:00, 8288.81it/s] 1617it [00:00, 8476.75it/s] 1617it [00:00, 8776.90it/s] 1617it [00:00, 9467.37it/s] 1617it [00:00, 9579.38it/s] 1617it [00:00, 9405.31it/s] 1617it [00:00, 8639.31it/s] 1617it [00:00, 8596.24it/s] 1617it [00:00, 8538.37it/s] 1617it [00:00, 9105.70it/s] 1617it [00:00, 8869.80it/s] 1617it [00:00, 7665.70it/s] 1617it [00:00, 9406.62it/s] 1617it [00:00, 9726.43it/s] 1617it [00:00, 9417.72it/s] 1617it [00:00, 9162.55it/s] 1617it [00:00, 9288.11it/s] 1617it [00:00, 9183.26it/s] 1617it [00:00, 9149.00it/s] 1617it [00:00, 8791.07it/s] 1617it [00:00, 8842.39it/s] 1617it [00:00, 9279.17it/s] 1617it [00:00, 9389.50it/s] 1617it [00:00, 8657.41it/s] 1617it [00:00, 9049.60it/s] 1617it [00:00, 9675.05it/s] 1617it [00:00, 9367.64it/s] 1617it [00:00, 9238.52it/s] 1617it [00:00, 7418.38it/s] 1617it [00:00, 5746.97it/s] 1617it [00:00, 7992.78it/s] 1617it [00:00, 6606.99it/s] 1617it [00:00, 9130.24it/s] 1617it [00:00, 8933.36it/s] 1617it [00:00, 8811.96it/s] 1617it [00:00, 9018.10it/s] 1617it [00:00, 9130.47it/s] 1617it [00:00, 9236.86it/s] 1617it [00:00, 7730.59it/s] 1617it [00:00, 9486.96it/s] 1617it [00:00, 8474.33it/s] 1617it [00:00, 9344.43it/s] 1617it [00:00, 9065.39it/s] 1617it [00:00, 9025.59it/s] 1617it [00:00, 9061.02it/s] 1617it [00:00, 9350.40it/s] . # a_dictionary . length_of_lists = [] for key, value in a_dictionary.items(): a = len(value) length_of_lists.append(a) . maximum = max(length_of_lists) . maximum = 61 . state_counter = [] cases_counter = [] for key, value in a_dictionary.items(): if len(value) == 0: new = [0] * maximum value = new else: new = value + [value[-1]] * (maximum-len(value)) value = new state_counter.append(key) cases_counter.append(value) . cases_counter[0] . [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0] . new_df = pd.DataFrame() . # new_df[&#39;Date&#39;] = all_dates . for f, b in zip(state_counter, cases_counter): new_df[f] = b . new_df . Chicago, IL San Benito, CA Santa Clara, CA Boston, MA Los Angeles, CA Madison, WI Orange, CA Seattle, WA Tempe, AZ Unassigned Location (From Diamond Princess) ... South Dakota West Virginia Wyoming Alabama Puerto Rico Virgin Islands, U.S. Guam Virgin Islands United States Virgin Islands US . 0 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 36.0 | ... | 0.0 | 0.0 | 0.0 | 5.0 | 3.0 | 1.0 | 3.0 | 1.0 | 2.0 | 1.0 | . 1 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 36.0 | ... | 8.0 | 0.0 | 0.0 | 6.0 | 5.0 | 1.0 | 3.0 | 2.0 | 2.0 | 1.0 | . 2 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 42.0 | ... | 8.0 | 0.0 | 1.0 | 12.0 | 5.0 | 1.0 | 3.0 | 2.0 | 3.0 | 1.0 | . 3 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 42.0 | ... | 8.0 | 0.0 | 1.0 | 29.0 | 5.0 | 1.0 | 5.0 | 3.0 | 6.0 | 1.0 | . 4 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 44.0 | ... | 9.0 | 0.0 | 2.0 | 39.0 | 5.0 | 1.0 | 12.0 | 3.0 | 6.0 | 1.0 | . 5 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 44.0 | ... | 9.0 | 0.0 | 3.0 | 46.0 | 5.0 | 1.0 | 14.0 | 3.0 | 6.0 | 1.0 | . 6 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 44.0 | ... | 10.0 | 0.0 | 3.0 | 78.0 | 14.0 | 1.0 | 15.0 | 3.0 | 6.0 | 1.0 | . 7 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 11.0 | 1.0 | 11.0 | 83.0 | 21.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 8 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 11.0 | 1.0 | 15.0 | 131.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 9 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 11.0 | 2.0 | 18.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 10 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 14.0 | 7.0 | 19.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 11 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 14.0 | 8.0 | 23.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 12 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 13 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 14 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 15 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 16 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 17 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 18 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 19 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 20 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 21 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 22 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 23 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 24 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 25 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 26 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 27 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 28 2.0 | 2.0 | 3.0 | 1.0 | 1.0 | 1.0 | 1.0 | 6.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 29 3.0 | 2.0 | 3.0 | 1.0 | 1.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 31 3.0 | 2.0 | 11.0 | 1.0 | 1.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 32 3.0 | 2.0 | 11.0 | 1.0 | 7.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 33 3.0 | 2.0 | 20.0 | 1.0 | 11.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 34 3.0 | 2.0 | 20.0 | 1.0 | 13.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 35 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 36 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 37 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 38 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 39 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 40 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 41 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 42 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 43 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 44 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 45 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 46 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 47 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 48 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 49 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 50 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 51 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 52 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 53 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 54 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 55 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 56 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 57 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 58 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 59 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 60 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 61 rows × 195 columns . new_date_df = pd.DataFrame() . new_date_df[&quot;Date&quot;] = all_dates . u = [] for i in new_df: print(i) . Chicago, IL San Benito, CA Santa Clara, CA Boston, MA Los Angeles, CA Madison, WI Orange, CA Seattle, WA Tempe, AZ Unassigned Location (From Diamond Princess) San Diego County, CA Humboldt County, CA Sacramento County, CA San Antonio, TX Lackland, TX (From Diamond Princess) Omaha, NE (From Diamond Princess) Travis, CA (From Diamond Princess) Washington Chicago Illinois California Arizona Ashland, NE Travis, CA Lackland, TX Portland, OR Snohomish County, WA Providence, RI King County, WA Cook County, IL Grafton County, NH Hillsborough, FL New York City, NY Placer County, CA San Mateo, CA Sarasota, FL Sonoma County, CA Umatilla, OR Fulton County, GA Washington County, OR Norfolk County, MA Berkeley, CA Maricopa County, AZ Wake County, NC Westchester County, NY Orange County, CA Contra Costa County, CA Bergen County, NJ Harris County, TX San Francisco County, CA Clark County, NV Fort Bend County, TX Grant County, WA Queens County, NY Santa Rosa County, FL Williamson County, TN New York County, NY Unassigned Location, WA Montgomery County, MD Suffolk County, MA Denver County, CO Summit County, CO Chatham County, NC Delaware County, PA Douglas County, NE Fayette County, KY Floyd County, GA Marion County, IN Middlesex County, MA Nassau County, NY Norwell County, MA Ramsey County, MN Washoe County, NV Wayne County, PA Yolo County, CA Santa Clara County, CA Grand Princess Cruise Ship Douglas County, CO Providence County, RI Alameda County, CA Broward County, FL Fairfield County, CT Lee County, FL Pinal County, AZ Rockland County, NY Saratoga County, NY Charleston County, SC Clark County, WA Cobb County, GA Davis County, UT El Paso County, CO Honolulu County, HI Jackson County, OR Jefferson County, WA Kershaw County, SC Klamath County, OR Madera County, CA Pierce County, WA Plymouth County, MA Santa Cruz County, CA Tulsa County, OK Montgomery County, TX Norfolk County, MA Montgomery County, PA Fairfax County, VA Rockingham County, NH Washington, D.C. Berkshire County, MA Davidson County, TN Douglas County, OR Fresno County, CA Harford County, MD Hendricks County, IN Hudson County, NJ Johnson County, KS Kittitas County, WA Manatee County, FL Marion County, OR Okaloosa County, FL Polk County, GA Riverside County, CA Shelby County, TN Spokane County, WA St. Louis County, MO Suffolk County, NY Ulster County, NY Unassigned Location, VT Unknown Location, MA Volusia County, FL Johnson County, IA Harrison County, KY Bennington County, VT Carver County, MN Charlotte County, FL Cherokee County, GA Collin County, TX Jefferson County, KY Jefferson Parish, LA Shasta County, CA Spartanburg County, SC New York Massachusetts Diamond Princess Grand Princess Georgia Colorado Florida New Jersey Oregon Texas Pennsylvania Iowa Maryland North Carolina South Carolina Tennessee Virginia Indiana Kentucky District of Columbia Nevada New Hampshire Minnesota Nebraska Ohio Rhode Island Wisconsin Connecticut Hawaii Oklahoma Utah Kansas Louisiana Missouri Vermont Alaska Arkansas Delaware Idaho Maine Michigan Mississippi Montana New Mexico North Dakota South Dakota West Virginia Wyoming Alabama Puerto Rico Virgin Islands, U.S. Guam Virgin Islands United States Virgin Islands US . ax = plt.figure(figsize=(20,6)) # Add title ax = plt.title(&quot;Confirmed Cases and Death Count due to COVID-19 in USA&quot;) for i in tqdm(new_df): ax = sns.lineplot(x=new_date_df[&#39;Date&#39;], y=new_df[i], label = str(i)) # ax = sns.lineplot(x = bd_df[&#39;date&#39;], y = bd_df[&#39;death&#39;], color = &#39;red&#39;, label = &quot;Death Count&quot;) ax.legend() # ax = sns.color_palette(&quot;RdBu&quot;, n_colors=7) ax.set_xticklabels(labels=new_date_df[&#39;Date&#39;], rotation=45, ha=&#39;right&#39;) ax = plt.ylabel(&quot;Count&quot;) . 0%| | 0/61 [00:00&lt;?, ?it/s] 2%|▏ | 1/61 [00:00&lt;00:07, 8.31it/s] 8%|▊ | 5/61 [00:00&lt;00:05, 10.68it/s] 13%|█▎ | 8/61 [00:00&lt;00:04, 13.14it/s] 18%|█▊ | 11/61 [00:00&lt;00:03, 15.61it/s] 23%|██▎ | 14/61 [00:00&lt;00:02, 17.79it/s] 28%|██▊ | 17/61 [00:00&lt;00:02, 19.44it/s] 33%|███▎ | 20/61 [00:00&lt;00:02, 20.44it/s] 38%|███▊ | 23/61 [00:00&lt;00:01, 21.23it/s] 43%|████▎ | 26/61 [00:01&lt;00:01, 21.29it/s] 48%|████▊ | 29/61 [00:01&lt;00:01, 20.95it/s] 52%|█████▏ | 32/61 [00:01&lt;00:01, 20.67it/s] 57%|█████▋ | 35/61 [00:01&lt;00:01, 19.36it/s] 61%|██████ | 37/61 [00:01&lt;00:01, 18.97it/s] 64%|██████▍ | 39/61 [00:01&lt;00:01, 17.95it/s] 67%|██████▋ | 41/61 [00:01&lt;00:01, 17.84it/s] 70%|███████ | 43/61 [00:02&lt;00:01, 17.70it/s] 74%|███████▍ | 45/61 [00:02&lt;00:00, 16.85it/s] 77%|███████▋ | 47/61 [00:02&lt;00:00, 16.10it/s] 80%|████████ | 49/61 [00:02&lt;00:00, 16.15it/s] 84%|████████▎ | 51/61 [00:02&lt;00:00, 16.10it/s] 87%|████████▋ | 53/61 [00:02&lt;00:00, 16.08it/s] 90%|█████████ | 55/61 [00:02&lt;00:00, 15.35it/s] 93%|█████████▎| 57/61 [00:02&lt;00:00, 14.51it/s] 97%|█████████▋| 59/61 [00:03&lt;00:00, 13.43it/s] 100%|██████████| 61/61 [00:03&lt;00:00, 13.46it/s] 63it [00:03, 12.39it/s] 65it [00:03, 11.54it/s] 67it [00:03, 11.20it/s] 69it [00:04, 11.75it/s] 71it [00:04, 12.13it/s] 73it [00:05, 3.56it/s] 75it [00:05, 4.53it/s] 77it [00:05, 5.63it/s] 79it [00:06, 6.77it/s] 81it [00:06, 7.88it/s] 83it [00:06, 8.50it/s] 85it [00:06, 9.16it/s] 87it [00:07, 7.20it/s] 89it [00:07, 8.00it/s] 91it [00:07, 8.84it/s] 93it [00:07, 9.51it/s] 95it [00:07, 9.66it/s] 97it [00:07, 10.09it/s] 99it [00:08, 10.28it/s] 101it [00:08, 7.32it/s] 103it [00:08, 8.00it/s] 104it [00:08, 8.01it/s] 105it [00:09, 8.42it/s] 106it [00:09, 8.75it/s] 107it [00:09, 8.99it/s] 108it [00:09, 8.59it/s] 109it [00:09, 7.74it/s] 110it [00:09, 7.68it/s] 111it [00:09, 6.85it/s] 112it [00:10, 4.62it/s] 113it [00:10, 5.10it/s] 114it [00:10, 5.45it/s] 115it [00:10, 5.98it/s] 116it [00:10, 6.09it/s] 117it [00:11, 5.85it/s] 118it [00:11, 6.08it/s] 119it [00:11, 6.12it/s] 120it [00:11, 6.35it/s] 121it [00:11, 6.71it/s] 122it [00:11, 6.32it/s] 123it [00:11, 6.61it/s] 124it [00:12, 3.93it/s] 125it [00:12, 4.38it/s] 126it [00:12, 5.00it/s] 127it [00:12, 5.43it/s] 128it [00:12, 6.14it/s] 129it [00:13, 6.73it/s] 130it [00:13, 7.28it/s] 131it [00:13, 7.47it/s] 132it [00:13, 7.43it/s] 133it [00:13, 7.56it/s] 134it [00:13, 7.57it/s] 135it [00:13, 7.72it/s] 136it [00:14, 4.26it/s] 137it [00:14, 4.89it/s] 138it [00:14, 5.39it/s] 139it [00:14, 5.89it/s] 140it [00:14, 6.31it/s] 141it [00:15, 6.48it/s] 142it [00:15, 6.70it/s] 143it [00:15, 6.99it/s] 144it [00:15, 6.97it/s] 145it [00:15, 6.75it/s] 146it [00:15, 7.10it/s] 147it [00:15, 7.05it/s] 148it [00:16, 3.83it/s] 149it [00:16, 4.51it/s] 150it [00:16, 5.13it/s] 151it [00:16, 5.61it/s] 152it [00:16, 5.81it/s] 153it [00:17, 6.11it/s] 154it [00:17, 6.48it/s] 155it [00:17, 6.56it/s] 156it [00:17, 6.71it/s] 157it [00:17, 6.75it/s] 158it [00:17, 6.65it/s] 159it [00:17, 6.79it/s] 160it [00:18, 3.70it/s] 161it [00:18, 4.39it/s] 162it [00:18, 4.95it/s] 163it [00:18, 5.46it/s] 164it [00:19, 5.88it/s] 165it [00:19, 6.33it/s] 166it [00:19, 6.70it/s] 167it [00:19, 6.94it/s] 168it [00:19, 7.16it/s] 169it [00:19, 7.23it/s] 170it [00:19, 7.02it/s] 171it [00:20, 7.12it/s] 172it [00:20, 3.66it/s] 173it [00:20, 4.26it/s] 174it [00:20, 4.84it/s] 175it [00:21, 5.36it/s] 176it [00:21, 5.60it/s] 177it [00:21, 5.71it/s] 178it [00:21, 5.86it/s] 179it [00:21, 6.20it/s] 180it [00:21, 6.39it/s] 181it [00:21, 6.48it/s] 182it [00:22, 6.32it/s] 183it [00:22, 6.27it/s] 184it [00:22, 3.33it/s] 185it [00:23, 3.95it/s] 186it [00:23, 4.54it/s] 187it [00:23, 5.06it/s] 188it [00:23, 5.43it/s] 189it [00:23, 5.71it/s] 190it [00:23, 5.90it/s] 191it [00:23, 6.10it/s] 192it [00:24, 6.22it/s] 193it [00:24, 6.34it/s] 194it [00:24, 6.47it/s] 195it [00:25, 3.25it/s] . ax = plt.figure(figsize=(20,6)) # Add title ax = plt.title(&quot;Confirmed Cases of COVID-19 in Chicago&quot;) # for i in tqdm(new_df): # ax = sns.lineplot(x=new_date_df[&#39;Date&#39;], y=new_df[i], label = str(i)) ax = sns.lineplot(x = new_date_df[&#39;Date&#39;], y = new_df[&#39;Chicago, IL&#39;], color = &#39;red&#39;, label = &quot;Confirmed Cases&quot;) ax.legend() # ax = sns.color_palette(&quot;RdBu&quot;, n_colors=7) ax.set_xticklabels(labels=new_date_df[&#39;Date&#39;], rotation=45, ha=&#39;right&#39;) ax = plt.ylabel(&quot;Count&quot;) .",
            "url": "https://thedrowsywinger.github.io/Analyzing-COVID-19-Data/2020/02/23/Trends-in-Top-Countries.html",
            "relUrl": "/2020/02/23/Trends-in-Top-Countries.html",
            "date": " • Feb 23, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc: true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://thedrowsywinger.github.io/Analyzing-COVID-19-Data/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Title",
            "content": "This dataset has been taken from: https://github.com/CSSEGISandData/COVID-19 . import pandas as pd import os from collections import Counter from tqdm import tqdm %matplotlib inline import matplotlib.pyplot as plt import seaborn as sns . cd /media/thedrowsywinger/2A24A59224A56195/Poralekha/github/COVID-19/csse_covid_19_data/csse_covid_19_daily_reports . /media/thedrowsywinger/2A24A59224A56195/Poralekha/github/COVID-19/csse_covid_19_data/csse_covid_19_daily_reports . ls . 01-22-2020.csv* 02-07-2020.csv* 02-23-2020.csv* 03-10-2020.csv* 01-23-2020.csv* 02-08-2020.csv* 02-24-2020.csv* 03-11-2020.csv* 01-24-2020.csv* 02-09-2020.csv* 02-25-2020.csv* 03-12-2020.csv* 01-25-2020.csv* 02-10-2020.csv* 02-26-2020.csv* 03-13-2020.csv* 01-26-2020.csv* 02-11-2020.csv* 02-27-2020.csv* 03-14-2020.csv* 01-27-2020.csv* 02-12-2020.csv* 02-28-2020.csv* 03-15-2020.csv* 01-28-2020.csv* 02-13-2020.csv* 02-29-2020.csv* 03-16-2020.csv* 01-29-2020.csv* 02-14-2020.csv* 03-01-2020.csv* 03-17-2020.csv* 01-30-2020.csv* 02-15-2020.csv* 03-02-2020.csv* 03-18-2020.csv* 01-31-2020.csv* 02-16-2020.csv* 03-03-2020.csv* 03-19-2020.csv* 02-01-2020.csv* 02-17-2020.csv* 03-04-2020.csv* 03-20-2020.csv* 02-02-2020.csv* 02-18-2020.csv* 03-05-2020.csv* 03-21-2020.csv* 02-03-2020.csv* 02-19-2020.csv* 03-06-2020.csv* 03-22-2020.csv* 02-04-2020.csv* 02-20-2020.csv* 03-07-2020.csv* README.md* 02-05-2020.csv* 02-21-2020.csv* 03-08-2020.csv* 02-06-2020.csv* 02-22-2020.csv* 03-09-2020.csv* . a = pd.read_csv(&quot;/media/thedrowsywinger/2A24A59224A56195/Poralekha/github/COVID-19/csse_covid_19_data/csse_covid_19_daily_reports/01-23-2020.csv&quot;) . What each CSV file looks like . a.head() . Province/State Country/Region Last Update Confirmed Deaths Recovered . 0 Anhui | Mainland China | 1/23/20 17:00 | 9.0 | NaN | NaN | . 1 Beijing | Mainland China | 1/23/20 17:00 | 22.0 | NaN | NaN | . 2 Chongqing | Mainland China | 1/23/20 17:00 | 9.0 | NaN | NaN | . 3 Fujian | Mainland China | 1/23/20 17:00 | 5.0 | NaN | NaN | . 4 Gansu | Mainland China | 1/23/20 17:00 | 2.0 | NaN | NaN | . province_list = [] for i in a[&#39;Province/State&#39;]: province_list.append(i) . len(province_list) . 46 . province_counts = Counter(province_list) . province_counts . Counter({&#39;Anhui&#39;: 1, &#39;Beijing&#39;: 1, &#39;Chongqing&#39;: 1, &#39;Fujian&#39;: 1, &#39;Gansu&#39;: 1, &#39;Guangdong&#39;: 1, &#39;Guangxi&#39;: 1, &#39;Guizhou&#39;: 1, &#39;Hainan&#39;: 1, &#39;Hebei&#39;: 1, &#39;Heilongjiang&#39;: 1, &#39;Henan&#39;: 1, &#39;Hong Kong&#39;: 1, &#39;Hubei&#39;: 1, &#39;Hunan&#39;: 1, &#39;Inner Mongolia&#39;: 1, &#39;Jiangsu&#39;: 1, &#39;Jiangxi&#39;: 1, &#39;Jilin&#39;: 1, &#39;Liaoning&#39;: 1, &#39;Macau&#39;: 1, &#39;Ningxia&#39;: 1, &#39;Qinghai&#39;: 1, &#39;Shaanxi&#39;: 1, &#39;Shandong&#39;: 1, &#39;Shanghai&#39;: 1, &#39;Shanxi&#39;: 1, &#39;Sichuan&#39;: 1, &#39;Taiwan&#39;: 1, &#39;Tianjin&#39;: 1, &#39;Tibet&#39;: 1, &#39;Washington&#39;: 1, &#39;Xinjiang&#39;: 1, &#39;Yunnan&#39;: 1, &#39;Zhejiang&#39;: 1, nan: 11}) . list_of_all_files = os.listdir() . # list_of_all_files . actual_list = [] for i in list_of_all_files: if &quot;csv&quot; in i: actual_list.append(i) . main = [] province_list = [] country_list = [] confirmed = [] death_count = [] date_list = [] for j in tqdm(actual_list): a = pd.read_csv(j) for i in range(len(a)): date_list.append(&quot;Date: &quot; + str(j[:-4])) for i in a[&#39;Province/State&#39;]: province_list.append(i) for i in a[&#39;Country/Region&#39;]: country_list.append(i) for i in a[&#39;Confirmed&#39;]: confirmed.append(i) for i in a[&#39;Deaths&#39;]: death_count.append(i) . 100%|██████████| 61/61 [00:00&lt;00:00, 291.88it/s] . main_df = pd.DataFrame({ &#39;date&#39;: date_list, &#39;state&#39;: province_list, &#39;country&#39;:country_list, &#39;confirmed_case&#39;: confirmed, &#39;death&#39;: death_count }) . len(death_count) . 7926 . Counter(country_list)[&#39;Bangladesh&#39;] . 15 . total_confirmed_case = 0.0 case_of_bd = [] case_of_death_bd = [] date_of_bd = [] for index, row in main_df.iterrows(): if row[&#39;country&#39;] == &quot;Bangladesh&quot;: print(row[&#39;date&#39;], &quot; Confirmed Cases: &quot;, row[&#39;confirmed_case&#39;], &quot; Confirmed Death: &quot;, row[&#39;death&#39;]) total_confirmed_case += row[&#39;confirmed_case&#39;] case_of_bd.append(row[&#39;confirmed_case&#39;]) case_of_death_bd.append(row[&#39;death&#39;]) date_of_bd.append(row[&#39;date&#39;]) # print(total_confirmed_case) bd_df = pd.DataFrame({ &#39;date&#39;: date_of_bd, &#39;confirmed_case&#39;: case_of_bd, &#39;death&#39;: case_of_death_bd }) . Date: 03-08-2020 Confirmed Cases: 3.0 Confirmed Death: 0.0 Date: 03-09-2020 Confirmed Cases: 3.0 Confirmed Death: 0.0 Date: 03-10-2020 Confirmed Cases: 3.0 Confirmed Death: 0.0 Date: 03-11-2020 Confirmed Cases: 3.0 Confirmed Death: 0.0 Date: 03-12-2020 Confirmed Cases: 3.0 Confirmed Death: 0.0 Date: 03-13-2020 Confirmed Cases: 3.0 Confirmed Death: 0.0 Date: 03-14-2020 Confirmed Cases: 3.0 Confirmed Death: 0.0 Date: 03-15-2020 Confirmed Cases: 5.0 Confirmed Death: 0.0 Date: 03-16-2020 Confirmed Cases: 8.0 Confirmed Death: 0.0 Date: 03-17-2020 Confirmed Cases: 10.0 Confirmed Death: 0.0 Date: 03-18-2020 Confirmed Cases: 14.0 Confirmed Death: 1.0 Date: 03-19-2020 Confirmed Cases: 17.0 Confirmed Death: 1.0 Date: 03-20-2020 Confirmed Cases: 20.0 Confirmed Death: 1.0 Date: 03-21-2020 Confirmed Cases: 25.0 Confirmed Death: 2.0 Date: 03-22-2020 Confirmed Cases: 27.0 Confirmed Death: 2.0 . ax = plt.figure(figsize=(20,10)) # Add title ax = plt.title(&quot;Confirmed Cases in Bangladesh&quot;) ax = sns.barplot(y=bd_df[&#39;confirmed_case&#39;], x=bd_df[&#39;date&#39;], saturation = 0.4, color=&quot;red&quot;) # ax = sns.color_palette(&quot;RdBu&quot;, n_colors=7) ax.set_xticklabels(labels=bd_df[&#39;date&#39;], rotation=45, ha=&#39;right&#39;) ax = plt.ylabel(&quot;Count&quot;) . ax = plt.figure(figsize=(20,10)) # Add title ax = plt.title(&quot;Confirmed Cases and Death Count due to COVID-19 in Bangladesh&quot;) ax = sns.barplot(y=bd_df[&#39;confirmed_case&#39;], x=bd_df[&#39;date&#39;], saturation = 0.4, color = &#39;Blue&#39;, label = &quot;Confirmed Case&quot;) ax = sns.barplot(x = bd_df[&#39;date&#39;], y = bd_df[&#39;death&#39;], color = &#39;red&#39;, label = &quot;Death Count&quot;) ax.legend() # ax = sns.color_palette(&quot;RdBu&quot;, n_colors=7) ax.set_xticklabels(labels=bd_df[&#39;date&#39;], rotation=45, ha=&#39;right&#39;) ax = plt.ylabel(&quot;Count&quot;) . Counter(country_list).most_common(30) . [(&#39;US&#39;, 1617), (&#39;Mainland China&#39;, 1517), (&#39;China&#39;, 396), (&#39;Australia&#39;, 323), (&#39;Canada&#39;, 254), (&#39;France&#39;, 127), (&#39;Japan&#39;, 61), (&#39;Thailand&#39;, 61), (&#39;Singapore&#39;, 60), (&#39;Malaysia&#39;, 59), (&#39;Vietnam&#39;, 59), (&#39;Nepal&#39;, 58), (&#39;Cambodia&#39;, 56), (&#39;Sri Lanka&#39;, 56), (&#39;Germany&#39;, 55), (&#39;United Kingdom&#39;, 55), (&#39;United Arab Emirates&#39;, 54), (&#39;Philippines&#39;, 54), (&#39;Finland&#39;, 54), (&#39;India&#39;, 53), (&#39;Italy&#39;, 52), (&#39;Sweden&#39;, 52), (&#39;Russia&#39;, 51), (&#39;Spain&#39;, 51), (&#39;Hong Kong&#39;, 48), (&#39;South Korea&#39;, 48), (&#39;Taiwan&#39;, 48), (&#39;Macau&#39;, 48), (&#39;Belgium&#39;, 48), (&#39;Denmark&#39;, 41)] . def specific_country(search): # total_confirmed_case = 0.0 case_of_confirmed = [] case_of_death = [] date = [] state = [] for index, row in main_df.iterrows(): if row[&#39;country&#39;] == search: case_of_confirmed.append(row[&#39;confirmed_case&#39;]) case_of_death.append(row[&#39;death&#39;]) date.append(row[&#39;date&#39;]) state.append(row[&#39;state&#39;]) # print(total_confirmed_case) bd_df = pd.DataFrame({ &#39;date&#39;: date, &#39;confirmed_case&#39;: case_of_confirmed, &#39;death&#39;: case_of_death, &#39;state&#39;: state }) return bd_df . bd_df = specific_country(&quot;Bangladesh&quot;) . ax = plt.figure(figsize=(20,10)) # Add title ax = plt.title(&quot;Confirmed Cases and Death Count due to COVID-19 in Bangladesh&quot;) ax = sns.lineplot(y=bd_df[&#39;confirmed_case&#39;], x=bd_df[&#39;date&#39;], color = &#39;Blue&#39;, label = &quot;Confirmed Case&quot;) ax = sns.lineplot(x = bd_df[&#39;date&#39;], y = bd_df[&#39;death&#39;], color = &#39;red&#39;, label = &quot;Death Count&quot;) ax.legend() # ax = sns.color_palette(&quot;RdBu&quot;, n_colors=7) ax.set_xticklabels(labels=bd_df[&#39;date&#39;], rotation=45, ha=&#39;right&#39;) ax = plt.ylabel(&quot;Count&quot;) . plt.figure(figsize=(14,6)) # Add title plt.title(&quot;Rise of COVID-19 in Bangladesh&quot;) sns.lineplot(data=bd_df[&#39;confirmed_case&#39;], label=&quot;Confirmed Case&quot;) sns.lineplot(data=bd_df[&#39;death&#39;], label=&quot;Death Toll&quot;, color = &quot;red&quot;) # Add label for horizontal axis plt.xlabel(&quot;Date&quot;) . Text(0.5, 0, &#39;Date&#39;) . us_df = specific_country(&quot;US&quot;) us_df.head() . date confirmed_case death state . 0 Date: 02-07-2020 | 2.0 | 0.0 | Chicago, IL | . 1 Date: 02-07-2020 | 2.0 | 0.0 | San Benito, CA | . 2 Date: 02-07-2020 | 2.0 | 0.0 | Santa Clara, CA | . 3 Date: 02-07-2020 | 1.0 | 0.0 | Boston, MA | . 4 Date: 02-07-2020 | 1.0 | 0.0 | Los Angeles, CA | . Counter(list(us_df[&#39;state&#39;])) . Counter({&#39;Chicago, IL&#39;: 30, &#39;San Benito, CA&#39;: 36, &#39;Santa Clara, CA&#39;: 35, &#39;Boston, MA&#39;: 34, &#39;Los Angeles, CA&#39;: 38, &#39;Madison, WI&#39;: 34, &#39;Orange, CA&#39;: 32, &#39;Seattle, WA&#39;: 30, &#39;Tempe, AZ&#39;: 35, &#39;Unassigned Location (From Diamond Princess)&#39;: 15, &#39;San Diego County, CA&#39;: 28, &#39;Humboldt County, CA&#39;: 18, &#39;Sacramento County, CA&#39;: 18, &#39;San Antonio, TX&#39;: 26, &#39;Lackland, TX (From Diamond Princess)&#39;: 17, &#39;Omaha, NE (From Diamond Princess)&#39;: 17, &#39;Travis, CA (From Diamond Princess)&#39;: 17, &#39;Washington&#39;: 23, &#39;Chicago&#39;: 1, &#39;Illinois&#39;: 20, &#39;California&#39;: 19, &#39;Arizona&#39;: 19, &#39;Ashland, NE&#39;: 1, &#39;Travis, CA&#39;: 1, &#39;Lackland, TX&#39;: 1, &#39;Portland, OR&#39;: 3, &#39;Snohomish County, WA&#39;: 10, &#39;Providence, RI&#39;: 6, &#39;King County, WA&#39;: 8, &#39;Cook County, IL&#39;: 8, &#39;Grafton County, NH&#39;: 8, &#39;Hillsborough, FL&#39;: 8, &#39;New York City, NY&#39;: 4, &#39;Placer County, CA&#39;: 8, &#39;San Mateo, CA&#39;: 8, &#39;Sarasota, FL&#39;: 8, &#39;Sonoma County, CA&#39;: 8, &#39;Umatilla, OR&#39;: 8, &#39;Fulton County, GA&#39;: 7, &#39;Washington County, OR&#39;: 7, &#39; Norfolk County, MA&#39;: 5, &#39;Berkeley, CA&#39;: 4, &#39;Maricopa County, AZ&#39;: 7, &#39;Wake County, NC&#39;: 7, &#39;Westchester County, NY&#39;: 7, &#39;Orange County, CA&#39;: 6, &#39;Contra Costa County, CA&#39;: 6, &#39;Bergen County, NJ&#39;: 5, &#39;Harris County, TX&#39;: 5, &#39;San Francisco County, CA&#39;: 5, &#39;Clark County, NV&#39;: 5, &#39;Fort Bend County, TX&#39;: 5, &#39;Grant County, WA&#39;: 5, &#39;Queens County, NY&#39;: 1, &#39;Santa Rosa County, FL&#39;: 5, &#39;Williamson County, TN&#39;: 5, &#39;New York County, NY&#39;: 4, &#39;Unassigned Location, WA&#39;: 4, &#39;Montgomery County, MD&#39;: 4, &#39;Suffolk County, MA&#39;: 4, &#39;Denver County, CO&#39;: 4, &#39;Summit County, CO&#39;: 4, &#39;Chatham County, NC&#39;: 4, &#39;Delaware County, PA&#39;: 4, &#39;Douglas County, NE&#39;: 4, &#39;Fayette County, KY&#39;: 4, &#39;Floyd County, GA&#39;: 2, &#39;Marion County, IN&#39;: 4, &#39;Middlesex County, MA&#39;: 4, &#39;Nassau County, NY&#39;: 4, &#39;Norwell County, MA&#39;: 1, &#39;Ramsey County, MN&#39;: 4, &#39;Washoe County, NV&#39;: 4, &#39;Wayne County, PA&#39;: 4, &#39;Yolo County, CA&#39;: 4, &#39;Santa Clara County, CA&#39;: 3, &#39;Grand Princess Cruise Ship&#39;: 3, &#39;Douglas County, CO&#39;: 3, &#39;Providence County, RI&#39;: 3, &#39;Alameda County, CA&#39;: 3, &#39;Broward County, FL&#39;: 3, &#39;Fairfield County, CT&#39;: 3, &#39;Lee County, FL&#39;: 3, &#39;Pinal County, AZ&#39;: 3, &#39;Rockland County, NY&#39;: 3, &#39;Saratoga County, NY&#39;: 3, &#39;Charleston County, SC&#39;: 3, &#39;Clark County, WA&#39;: 3, &#39;Cobb County, GA&#39;: 3, &#39;Davis County, UT&#39;: 3, &#39;El Paso County, CO&#39;: 3, &#39;Honolulu County, HI&#39;: 3, &#39;Jackson County, OR &#39;: 3, &#39;Jefferson County, WA&#39;: 3, &#39;Kershaw County, SC&#39;: 3, &#39;Klamath County, OR&#39;: 3, &#39;Madera County, CA&#39;: 3, &#39;Pierce County, WA&#39;: 3, &#39;Plymouth County, MA&#39;: 3, &#39;Santa Cruz County, CA&#39;: 1, &#39;Tulsa County, OK&#39;: 3, &#39;Montgomery County, TX&#39;: 3, &#39;Norfolk County, MA&#39;: 2, &#39;Montgomery County, PA&#39;: 2, &#39;Fairfax County, VA&#39;: 2, &#39;Rockingham County, NH&#39;: 2, &#39;Washington, D.C.&#39;: 2, &#39;Berkshire County, MA&#39;: 2, &#39;Davidson County, TN&#39;: 2, &#39;Douglas County, OR&#39;: 2, &#39;Fresno County, CA&#39;: 2, &#39;Harford County, MD&#39;: 2, &#39;Hendricks County, IN&#39;: 2, &#39;Hudson County, NJ&#39;: 2, &#39;Johnson County, KS&#39;: 2, &#39;Kittitas County, WA&#39;: 2, &#39;Manatee County, FL&#39;: 2, &#39;Marion County, OR&#39;: 2, &#39;Okaloosa County, FL&#39;: 2, &#39;Polk County, GA&#39;: 2, &#39;Riverside County, CA&#39;: 2, &#39;Shelby County, TN&#39;: 2, &#39;Spokane County, WA&#39;: 2, &#39;St. Louis County, MO&#39;: 2, &#39;Suffolk County, NY&#39;: 2, &#39;Ulster County, NY&#39;: 2, &#39;Unassigned Location, VT&#39;: 1, &#39;Unknown Location, MA&#39;: 2, &#39;Volusia County, FL&#39;: 2, &#39;Johnson County, IA&#39;: 1, &#39;Harrison County, KY&#39;: 1, &#39;Bennington County, VT&#39;: 1, &#39;Carver County, MN&#39;: 1, &#39;Charlotte County, FL&#39;: 1, &#39;Cherokee County, GA&#39;: 1, &#39;Collin County, TX&#39;: 1, &#39;Jefferson County, KY&#39;: 1, &#39;Jefferson Parish, LA&#39;: 1, &#39;Shasta County, CA&#39;: 1, &#39;Spartanburg County, SC&#39;: 1, &#39;New York&#39;: 13, &#39;Massachusetts&#39;: 13, &#39;Diamond Princess&#39;: 13, &#39;Grand Princess&#39;: 13, &#39;Georgia&#39;: 13, &#39;Colorado&#39;: 13, &#39;Florida&#39;: 13, &#39;New Jersey&#39;: 13, &#39;Oregon&#39;: 13, &#39;Texas&#39;: 13, &#39;Pennsylvania&#39;: 13, &#39;Iowa&#39;: 13, &#39;Maryland&#39;: 13, &#39;North Carolina&#39;: 13, &#39;South Carolina&#39;: 13, &#39;Tennessee&#39;: 13, &#39;Virginia&#39;: 13, &#39;Indiana&#39;: 13, &#39;Kentucky&#39;: 13, &#39;District of Columbia&#39;: 13, &#39;Nevada&#39;: 13, &#39;New Hampshire&#39;: 13, &#39;Minnesota&#39;: 13, &#39;Nebraska&#39;: 13, &#39;Ohio&#39;: 13, &#39;Rhode Island&#39;: 13, &#39;Wisconsin&#39;: 13, &#39;Connecticut&#39;: 13, &#39;Hawaii&#39;: 13, &#39;Oklahoma&#39;: 13, &#39;Utah&#39;: 13, &#39;Kansas&#39;: 13, &#39;Louisiana&#39;: 13, &#39;Missouri&#39;: 13, &#39;Vermont&#39;: 13, &#39;Alaska&#39;: 12, &#39;Arkansas&#39;: 13, &#39;Delaware&#39;: 13, &#39;Idaho&#39;: 13, &#39;Maine&#39;: 13, &#39;Michigan&#39;: 13, &#39;Mississippi&#39;: 13, &#39;Montana&#39;: 13, &#39;New Mexico&#39;: 13, &#39;North Dakota&#39;: 13, &#39;South Dakota&#39;: 13, &#39;West Virginia&#39;: 13, &#39;Wyoming&#39;: 13, &#39;Alabama&#39;: 10, &#39;Puerto Rico&#39;: 9, &#39;Virgin Islands, U.S.&#39;: 2, &#39;Guam&#39;: 8, &#39;Virgin Islands&#39;: 4, &#39;United States Virgin Islands&#39;: 5, &#39;US&#39;: 5}) . duplicates_included = list(us_df[&#39;state&#39;]) . duplicate_dates = list(us_df[&#39;date&#39;]) . h = duplicate_dates[6] . if h not in duplicate_dates: pass else: print(&quot;no&quot;) . no . all_dates = [] for x in duplicate_dates: if x not in all_dates: all_dates.append(x) . len(all_dates) . 61 . all_states = [] for x in duplicates_included: if x not in all_states: all_states.append(x) . new_df = pd.DataFrame() a_dictionary = {} for i in all_states: case_of_confirmed = [] date_list = [] for index, row in tqdm(us_df.iterrows()): if i == row[&#39;state&#39;]: case_of_confirmed.append(row[&#39;confirmed_case&#39;]) date_list.append(row[&#39;date&#39;]) a_dictionary[i] = case_of_confirmed . 1617it [00:00, 9897.03it/s] 1617it [00:00, 11750.16it/s] 1617it [00:00, 11742.24it/s] 1617it [00:00, 11434.92it/s] 1617it [00:00, 11893.80it/s] 1617it [00:00, 10751.23it/s] 1617it [00:00, 10138.73it/s] 1617it [00:00, 9356.25it/s] 1617it [00:00, 10347.46it/s] 1617it [00:00, 11522.09it/s] 1617it [00:00, 10398.69it/s] 1617it [00:00, 10219.87it/s] 1617it [00:00, 9235.74it/s] 1617it [00:00, 9264.53it/s] 1617it [00:00, 9003.03it/s] 1617it [00:00, 9227.36it/s] 1617it [00:00, 9227.12it/s] 1617it [00:00, 9034.51it/s] 1617it [00:00, 8195.64it/s] 1617it [00:00, 8527.91it/s] 1617it [00:00, 8160.34it/s] 1617it [00:00, 8660.94it/s] 1617it [00:00, 8609.87it/s] 1617it [00:00, 7859.78it/s] 1617it [00:00, 8486.59it/s] 1617it [00:00, 9129.04it/s] 1617it [00:00, 9170.49it/s] 1617it [00:00, 8998.69it/s] 1617it [00:00, 8724.91it/s] 1617it [00:00, 9067.24it/s] 1617it [00:00, 8660.79it/s] 1617it [00:00, 9139.96it/s] 1617it [00:00, 9242.53it/s] 1617it [00:00, 8815.31it/s] 1617it [00:00, 8323.04it/s] 1617it [00:00, 9277.44it/s] 1617it [00:00, 9151.53it/s] 1617it [00:00, 9443.72it/s] 1617it [00:00, 9101.01it/s] 1617it [00:00, 9480.81it/s] 1617it [00:00, 8805.89it/s] 1617it [00:00, 8654.21it/s] 1617it [00:00, 8923.72it/s] 1617it [00:00, 6763.55it/s] 1617it [00:00, 9370.28it/s] 1617it [00:00, 9316.31it/s] 1617it [00:00, 9550.51it/s] 1617it [00:00, 9002.05it/s] 1617it [00:00, 9469.38it/s] 1617it [00:00, 9253.00it/s] 1617it [00:00, 8832.69it/s] 1617it [00:00, 8076.19it/s] 1617it [00:00, 9120.41it/s] 1617it [00:00, 9088.79it/s] 1617it [00:00, 8280.46it/s] 1617it [00:00, 8369.44it/s] 1617it [00:00, 7817.37it/s] 1617it [00:00, 9069.88it/s] 1617it [00:00, 9476.14it/s] 1617it [00:00, 9548.61it/s] 1617it [00:00, 8863.21it/s] 1617it [00:00, 7681.98it/s] 1617it [00:00, 8134.70it/s] 1617it [00:00, 8906.50it/s] 1617it [00:00, 9616.28it/s] 1617it [00:00, 9422.23it/s] 1617it [00:00, 8871.82it/s] 1617it [00:00, 9173.40it/s] 1617it [00:00, 9087.24it/s] 1617it [00:00, 9593.06it/s] 1617it [00:00, 9458.01it/s] 1617it [00:00, 9704.04it/s] 1617it [00:00, 8506.53it/s] 1617it [00:00, 8809.24it/s] 1617it [00:00, 9324.11it/s] 1617it [00:00, 9138.52it/s] 1617it [00:00, 9295.73it/s] 1617it [00:00, 9470.56it/s] 1617it [00:00, 8840.73it/s] 1617it [00:00, 9301.09it/s] 1617it [00:00, 9254.12it/s] 1617it [00:00, 9076.39it/s] 1617it [00:00, 9507.97it/s] 1617it [00:00, 8809.27it/s] 1617it [00:00, 9387.24it/s] 1617it [00:00, 9530.12it/s] 1617it [00:00, 8957.30it/s] 1617it [00:00, 9329.12it/s] 1617it [00:00, 9365.45it/s] 1617it [00:00, 8436.51it/s] 1617it [00:00, 9220.87it/s] 1617it [00:00, 9055.25it/s] 1617it [00:00, 9792.71it/s] 1617it [00:00, 8957.36it/s] 1617it [00:00, 9159.69it/s] 1617it [00:00, 8534.27it/s] 1617it [00:00, 9510.67it/s] 1617it [00:00, 9482.52it/s] 1617it [00:00, 9440.95it/s] 1617it [00:00, 9413.64it/s] 1617it [00:00, 8774.43it/s] 1617it [00:00, 8972.99it/s] 1617it [00:00, 9310.22it/s] 1617it [00:00, 9486.76it/s] 1617it [00:00, 9711.50it/s] 1617it [00:00, 9616.58it/s] 1617it [00:00, 8461.64it/s] 1617it [00:00, 9290.35it/s] 1617it [00:00, 9442.05it/s] 1617it [00:00, 9490.33it/s] 1617it [00:00, 8717.96it/s] 1617it [00:00, 8403.14it/s] 1617it [00:00, 7958.63it/s] 1617it [00:00, 8583.46it/s] 1617it [00:00, 8710.26it/s] 1617it [00:00, 8792.19it/s] 1617it [00:00, 9056.12it/s] 1617it [00:00, 8133.16it/s] 1617it [00:00, 9167.86it/s] 1617it [00:00, 8401.98it/s] 1617it [00:00, 8888.65it/s] 1617it [00:00, 9304.73it/s] 1617it [00:00, 8224.59it/s] 1617it [00:00, 9238.79it/s] 1617it [00:00, 8862.06it/s] 1617it [00:00, 9083.52it/s] 1617it [00:00, 8734.28it/s] 1617it [00:00, 9274.50it/s] 1617it [00:00, 9002.36it/s] 1617it [00:00, 9638.61it/s] 1617it [00:00, 9430.83it/s] 1617it [00:00, 9472.25it/s] 1617it [00:00, 9332.68it/s] 1617it [00:00, 8101.39it/s] 1617it [00:00, 7268.92it/s] 1617it [00:00, 9203.90it/s] 1617it [00:00, 9422.46it/s] 1617it [00:00, 9346.25it/s] 1617it [00:00, 9427.41it/s] 1617it [00:00, 8463.77it/s] 1617it [00:00, 9705.91it/s] 1617it [00:00, 9152.91it/s] 1617it [00:00, 9367.32it/s] 1617it [00:00, 9272.37it/s] 1617it [00:00, 8533.95it/s] 1617it [00:00, 8920.42it/s] 1617it [00:00, 8939.04it/s] 1617it [00:00, 8863.05it/s] 1617it [00:00, 8606.84it/s] 1617it [00:00, 8288.81it/s] 1617it [00:00, 8476.75it/s] 1617it [00:00, 8776.90it/s] 1617it [00:00, 9467.37it/s] 1617it [00:00, 9579.38it/s] 1617it [00:00, 9405.31it/s] 1617it [00:00, 8639.31it/s] 1617it [00:00, 8596.24it/s] 1617it [00:00, 8538.37it/s] 1617it [00:00, 9105.70it/s] 1617it [00:00, 8869.80it/s] 1617it [00:00, 7665.70it/s] 1617it [00:00, 9406.62it/s] 1617it [00:00, 9726.43it/s] 1617it [00:00, 9417.72it/s] 1617it [00:00, 9162.55it/s] 1617it [00:00, 9288.11it/s] 1617it [00:00, 9183.26it/s] 1617it [00:00, 9149.00it/s] 1617it [00:00, 8791.07it/s] 1617it [00:00, 8842.39it/s] 1617it [00:00, 9279.17it/s] 1617it [00:00, 9389.50it/s] 1617it [00:00, 8657.41it/s] 1617it [00:00, 9049.60it/s] 1617it [00:00, 9675.05it/s] 1617it [00:00, 9367.64it/s] 1617it [00:00, 9238.52it/s] 1617it [00:00, 7418.38it/s] 1617it [00:00, 5746.97it/s] 1617it [00:00, 7992.78it/s] 1617it [00:00, 6606.99it/s] 1617it [00:00, 9130.24it/s] 1617it [00:00, 8933.36it/s] 1617it [00:00, 8811.96it/s] 1617it [00:00, 9018.10it/s] 1617it [00:00, 9130.47it/s] 1617it [00:00, 9236.86it/s] 1617it [00:00, 7730.59it/s] 1617it [00:00, 9486.96it/s] 1617it [00:00, 8474.33it/s] 1617it [00:00, 9344.43it/s] 1617it [00:00, 9065.39it/s] 1617it [00:00, 9025.59it/s] 1617it [00:00, 9061.02it/s] 1617it [00:00, 9350.40it/s] . # a_dictionary . length_of_lists = [] for key, value in a_dictionary.items(): a = len(value) length_of_lists.append(a) . maximum = max(length_of_lists) . maximum = 61 . state_counter = [] cases_counter = [] for key, value in a_dictionary.items(): if len(value) == 0: new = [0] * maximum value = new else: new = value + [value[-1]] * (maximum-len(value)) value = new state_counter.append(key) cases_counter.append(value) . cases_counter[0] . [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0] . new_df = pd.DataFrame() . # new_df[&#39;Date&#39;] = all_dates . for f, b in zip(state_counter, cases_counter): new_df[f] = b . new_df . Chicago, IL San Benito, CA Santa Clara, CA Boston, MA Los Angeles, CA Madison, WI Orange, CA Seattle, WA Tempe, AZ Unassigned Location (From Diamond Princess) ... South Dakota West Virginia Wyoming Alabama Puerto Rico Virgin Islands, U.S. Guam Virgin Islands United States Virgin Islands US . 0 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 36.0 | ... | 0.0 | 0.0 | 0.0 | 5.0 | 3.0 | 1.0 | 3.0 | 1.0 | 2.0 | 1.0 | . 1 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 36.0 | ... | 8.0 | 0.0 | 0.0 | 6.0 | 5.0 | 1.0 | 3.0 | 2.0 | 2.0 | 1.0 | . 2 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 42.0 | ... | 8.0 | 0.0 | 1.0 | 12.0 | 5.0 | 1.0 | 3.0 | 2.0 | 3.0 | 1.0 | . 3 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 42.0 | ... | 8.0 | 0.0 | 1.0 | 29.0 | 5.0 | 1.0 | 5.0 | 3.0 | 6.0 | 1.0 | . 4 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 44.0 | ... | 9.0 | 0.0 | 2.0 | 39.0 | 5.0 | 1.0 | 12.0 | 3.0 | 6.0 | 1.0 | . 5 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 44.0 | ... | 9.0 | 0.0 | 3.0 | 46.0 | 5.0 | 1.0 | 14.0 | 3.0 | 6.0 | 1.0 | . 6 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 44.0 | ... | 10.0 | 0.0 | 3.0 | 78.0 | 14.0 | 1.0 | 15.0 | 3.0 | 6.0 | 1.0 | . 7 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 11.0 | 1.0 | 11.0 | 83.0 | 21.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 8 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 11.0 | 1.0 | 15.0 | 131.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 9 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 11.0 | 2.0 | 18.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 10 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 14.0 | 7.0 | 19.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 11 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 14.0 | 8.0 | 23.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 12 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 13 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 14 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 15 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 16 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 17 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 18 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 19 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 20 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 21 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 22 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 23 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 24 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 25 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 26 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 27 2.0 | 2.0 | 2.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 28 2.0 | 2.0 | 3.0 | 1.0 | 1.0 | 1.0 | 1.0 | 6.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 29 3.0 | 2.0 | 3.0 | 1.0 | 1.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 31 3.0 | 2.0 | 11.0 | 1.0 | 1.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 32 3.0 | 2.0 | 11.0 | 1.0 | 7.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 33 3.0 | 2.0 | 20.0 | 1.0 | 11.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 34 3.0 | 2.0 | 20.0 | 1.0 | 13.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 35 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 36 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 37 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 38 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 39 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 40 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 41 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 42 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 43 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 44 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 45 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 46 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 47 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 48 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 49 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 50 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 51 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 52 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 53 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 54 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 55 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 56 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 57 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 58 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 59 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 60 3.0 | 2.0 | 20.0 | 1.0 | 14.0 | 1.0 | 1.0 | 9.0 | 1.0 | 45.0 | ... | 21.0 | 12.0 | 24.0 | 138.0 | 23.0 | 1.0 | 27.0 | 3.0 | 6.0 | 1.0 | . 61 rows × 195 columns . new_date_df = pd.DataFrame() . new_date_df[&quot;Date&quot;] = all_dates . u = [] for i in new_df: print(i) . Chicago, IL San Benito, CA Santa Clara, CA Boston, MA Los Angeles, CA Madison, WI Orange, CA Seattle, WA Tempe, AZ Unassigned Location (From Diamond Princess) San Diego County, CA Humboldt County, CA Sacramento County, CA San Antonio, TX Lackland, TX (From Diamond Princess) Omaha, NE (From Diamond Princess) Travis, CA (From Diamond Princess) Washington Chicago Illinois California Arizona Ashland, NE Travis, CA Lackland, TX Portland, OR Snohomish County, WA Providence, RI King County, WA Cook County, IL Grafton County, NH Hillsborough, FL New York City, NY Placer County, CA San Mateo, CA Sarasota, FL Sonoma County, CA Umatilla, OR Fulton County, GA Washington County, OR Norfolk County, MA Berkeley, CA Maricopa County, AZ Wake County, NC Westchester County, NY Orange County, CA Contra Costa County, CA Bergen County, NJ Harris County, TX San Francisco County, CA Clark County, NV Fort Bend County, TX Grant County, WA Queens County, NY Santa Rosa County, FL Williamson County, TN New York County, NY Unassigned Location, WA Montgomery County, MD Suffolk County, MA Denver County, CO Summit County, CO Chatham County, NC Delaware County, PA Douglas County, NE Fayette County, KY Floyd County, GA Marion County, IN Middlesex County, MA Nassau County, NY Norwell County, MA Ramsey County, MN Washoe County, NV Wayne County, PA Yolo County, CA Santa Clara County, CA Grand Princess Cruise Ship Douglas County, CO Providence County, RI Alameda County, CA Broward County, FL Fairfield County, CT Lee County, FL Pinal County, AZ Rockland County, NY Saratoga County, NY Charleston County, SC Clark County, WA Cobb County, GA Davis County, UT El Paso County, CO Honolulu County, HI Jackson County, OR Jefferson County, WA Kershaw County, SC Klamath County, OR Madera County, CA Pierce County, WA Plymouth County, MA Santa Cruz County, CA Tulsa County, OK Montgomery County, TX Norfolk County, MA Montgomery County, PA Fairfax County, VA Rockingham County, NH Washington, D.C. Berkshire County, MA Davidson County, TN Douglas County, OR Fresno County, CA Harford County, MD Hendricks County, IN Hudson County, NJ Johnson County, KS Kittitas County, WA Manatee County, FL Marion County, OR Okaloosa County, FL Polk County, GA Riverside County, CA Shelby County, TN Spokane County, WA St. Louis County, MO Suffolk County, NY Ulster County, NY Unassigned Location, VT Unknown Location, MA Volusia County, FL Johnson County, IA Harrison County, KY Bennington County, VT Carver County, MN Charlotte County, FL Cherokee County, GA Collin County, TX Jefferson County, KY Jefferson Parish, LA Shasta County, CA Spartanburg County, SC New York Massachusetts Diamond Princess Grand Princess Georgia Colorado Florida New Jersey Oregon Texas Pennsylvania Iowa Maryland North Carolina South Carolina Tennessee Virginia Indiana Kentucky District of Columbia Nevada New Hampshire Minnesota Nebraska Ohio Rhode Island Wisconsin Connecticut Hawaii Oklahoma Utah Kansas Louisiana Missouri Vermont Alaska Arkansas Delaware Idaho Maine Michigan Mississippi Montana New Mexico North Dakota South Dakota West Virginia Wyoming Alabama Puerto Rico Virgin Islands, U.S. Guam Virgin Islands United States Virgin Islands US . ax = plt.figure(figsize=(20,6)) # Add title ax = plt.title(&quot;Confirmed Cases and Death Count due to COVID-19 in USA&quot;) for i in tqdm(new_df): ax = sns.lineplot(x=new_date_df[&#39;Date&#39;], y=new_df[i], label = str(i)) # ax = sns.lineplot(x = bd_df[&#39;date&#39;], y = bd_df[&#39;death&#39;], color = &#39;red&#39;, label = &quot;Death Count&quot;) ax.legend() # ax = sns.color_palette(&quot;RdBu&quot;, n_colors=7) ax.set_xticklabels(labels=new_date_df[&#39;Date&#39;], rotation=45, ha=&#39;right&#39;) ax = plt.ylabel(&quot;Count&quot;) . 0%| | 0/61 [00:00&lt;?, ?it/s] 2%|▏ | 1/61 [00:00&lt;00:07, 8.31it/s] 8%|▊ | 5/61 [00:00&lt;00:05, 10.68it/s] 13%|█▎ | 8/61 [00:00&lt;00:04, 13.14it/s] 18%|█▊ | 11/61 [00:00&lt;00:03, 15.61it/s] 23%|██▎ | 14/61 [00:00&lt;00:02, 17.79it/s] 28%|██▊ | 17/61 [00:00&lt;00:02, 19.44it/s] 33%|███▎ | 20/61 [00:00&lt;00:02, 20.44it/s] 38%|███▊ | 23/61 [00:00&lt;00:01, 21.23it/s] 43%|████▎ | 26/61 [00:01&lt;00:01, 21.29it/s] 48%|████▊ | 29/61 [00:01&lt;00:01, 20.95it/s] 52%|█████▏ | 32/61 [00:01&lt;00:01, 20.67it/s] 57%|█████▋ | 35/61 [00:01&lt;00:01, 19.36it/s] 61%|██████ | 37/61 [00:01&lt;00:01, 18.97it/s] 64%|██████▍ | 39/61 [00:01&lt;00:01, 17.95it/s] 67%|██████▋ | 41/61 [00:01&lt;00:01, 17.84it/s] 70%|███████ | 43/61 [00:02&lt;00:01, 17.70it/s] 74%|███████▍ | 45/61 [00:02&lt;00:00, 16.85it/s] 77%|███████▋ | 47/61 [00:02&lt;00:00, 16.10it/s] 80%|████████ | 49/61 [00:02&lt;00:00, 16.15it/s] 84%|████████▎ | 51/61 [00:02&lt;00:00, 16.10it/s] 87%|████████▋ | 53/61 [00:02&lt;00:00, 16.08it/s] 90%|█████████ | 55/61 [00:02&lt;00:00, 15.35it/s] 93%|█████████▎| 57/61 [00:02&lt;00:00, 14.51it/s] 97%|█████████▋| 59/61 [00:03&lt;00:00, 13.43it/s] 100%|██████████| 61/61 [00:03&lt;00:00, 13.46it/s] 63it [00:03, 12.39it/s] 65it [00:03, 11.54it/s] 67it [00:03, 11.20it/s] 69it [00:04, 11.75it/s] 71it [00:04, 12.13it/s] 73it [00:05, 3.56it/s] 75it [00:05, 4.53it/s] 77it [00:05, 5.63it/s] 79it [00:06, 6.77it/s] 81it [00:06, 7.88it/s] 83it [00:06, 8.50it/s] 85it [00:06, 9.16it/s] 87it [00:07, 7.20it/s] 89it [00:07, 8.00it/s] 91it [00:07, 8.84it/s] 93it [00:07, 9.51it/s] 95it [00:07, 9.66it/s] 97it [00:07, 10.09it/s] 99it [00:08, 10.28it/s] 101it [00:08, 7.32it/s] 103it [00:08, 8.00it/s] 104it [00:08, 8.01it/s] 105it [00:09, 8.42it/s] 106it [00:09, 8.75it/s] 107it [00:09, 8.99it/s] 108it [00:09, 8.59it/s] 109it [00:09, 7.74it/s] 110it [00:09, 7.68it/s] 111it [00:09, 6.85it/s] 112it [00:10, 4.62it/s] 113it [00:10, 5.10it/s] 114it [00:10, 5.45it/s] 115it [00:10, 5.98it/s] 116it [00:10, 6.09it/s] 117it [00:11, 5.85it/s] 118it [00:11, 6.08it/s] 119it [00:11, 6.12it/s] 120it [00:11, 6.35it/s] 121it [00:11, 6.71it/s] 122it [00:11, 6.32it/s] 123it [00:11, 6.61it/s] 124it [00:12, 3.93it/s] 125it [00:12, 4.38it/s] 126it [00:12, 5.00it/s] 127it [00:12, 5.43it/s] 128it [00:12, 6.14it/s] 129it [00:13, 6.73it/s] 130it [00:13, 7.28it/s] 131it [00:13, 7.47it/s] 132it [00:13, 7.43it/s] 133it [00:13, 7.56it/s] 134it [00:13, 7.57it/s] 135it [00:13, 7.72it/s] 136it [00:14, 4.26it/s] 137it [00:14, 4.89it/s] 138it [00:14, 5.39it/s] 139it [00:14, 5.89it/s] 140it [00:14, 6.31it/s] 141it [00:15, 6.48it/s] 142it [00:15, 6.70it/s] 143it [00:15, 6.99it/s] 144it [00:15, 6.97it/s] 145it [00:15, 6.75it/s] 146it [00:15, 7.10it/s] 147it [00:15, 7.05it/s] 148it [00:16, 3.83it/s] 149it [00:16, 4.51it/s] 150it [00:16, 5.13it/s] 151it [00:16, 5.61it/s] 152it [00:16, 5.81it/s] 153it [00:17, 6.11it/s] 154it [00:17, 6.48it/s] 155it [00:17, 6.56it/s] 156it [00:17, 6.71it/s] 157it [00:17, 6.75it/s] 158it [00:17, 6.65it/s] 159it [00:17, 6.79it/s] 160it [00:18, 3.70it/s] 161it [00:18, 4.39it/s] 162it [00:18, 4.95it/s] 163it [00:18, 5.46it/s] 164it [00:19, 5.88it/s] 165it [00:19, 6.33it/s] 166it [00:19, 6.70it/s] 167it [00:19, 6.94it/s] 168it [00:19, 7.16it/s] 169it [00:19, 7.23it/s] 170it [00:19, 7.02it/s] 171it [00:20, 7.12it/s] 172it [00:20, 3.66it/s] 173it [00:20, 4.26it/s] 174it [00:20, 4.84it/s] 175it [00:21, 5.36it/s] 176it [00:21, 5.60it/s] 177it [00:21, 5.71it/s] 178it [00:21, 5.86it/s] 179it [00:21, 6.20it/s] 180it [00:21, 6.39it/s] 181it [00:21, 6.48it/s] 182it [00:22, 6.32it/s] 183it [00:22, 6.27it/s] 184it [00:22, 3.33it/s] 185it [00:23, 3.95it/s] 186it [00:23, 4.54it/s] 187it [00:23, 5.06it/s] 188it [00:23, 5.43it/s] 189it [00:23, 5.71it/s] 190it [00:23, 5.90it/s] 191it [00:23, 6.10it/s] 192it [00:24, 6.22it/s] 193it [00:24, 6.34it/s] 194it [00:24, 6.47it/s] 195it [00:25, 3.25it/s] . ax = plt.figure(figsize=(20,6)) # Add title ax = plt.title(&quot;Confirmed Cases of COVID-19 in Chicago&quot;) # for i in tqdm(new_df): # ax = sns.lineplot(x=new_date_df[&#39;Date&#39;], y=new_df[i], label = str(i)) ax = sns.lineplot(x = new_date_df[&#39;Date&#39;], y = new_df[&#39;Chicago, IL&#39;], color = &#39;red&#39;, label = &quot;Confirmed Cases&quot;) ax.legend() # ax = sns.color_palette(&quot;RdBu&quot;, n_colors=7) ax.set_xticklabels(labels=new_date_df[&#39;Date&#39;], rotation=45, ha=&#39;right&#39;) ax = plt.ylabel(&quot;Count&quot;) .",
            "url": "https://thedrowsywinger.github.io/Analyzing-COVID-19-Data/2020/02/20/JHU-dataset.html",
            "relUrl": "/2020/02/20/JHU-dataset.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Title",
            "content": "EDA . import pandas as pd import seaborn as sns import json import matplotlib.pyplot as plt from tqdm import tqdm from collections import Counter import re from nltk.corpus import stopwords import seaborn as sns . reading_metadata = pd.read_csv(&quot;/media/thedrowsywinger/2A24A59224A56195/Poralekha/kaggle/covid/latter/CORD-19-research-challenge/metadata.csv&quot;) . title_list = [] for i in reading_metadata[&#39;title&#39;]: title_list.append(i) . len(title_list) . 44220 . words_list = [] for i in tqdm(title_list): # print(i) words = str(i).split(&quot; &quot;) for j in words: words_list.append(j.lower()) . 100%|██████████| 44220/44220 [00:00&lt;00:00, 232535.43it/s] . print(&#39;Number of words in text file :&#39;, len(words_list)) . Number of words in text file : 545436 . stop_words = set(stopwords.words(&#39;english&#39;)) filtered = [] for w in tqdm(words_list): if w not in stop_words: filtered.append(w) . 100%|██████████| 545436/545436 [00:00&lt;00:00, 1808284.80it/s] . frequency_dictionary = {} for keys in tqdm(filtered): frequency_dictionary[keys] = frequency_dictionary.get(keys, 0) + 1 . 100%|██████████| 401569/401569 [00:00&lt;00:00, 1477269.11it/s] . word_counts = Counter(filtered) w = word_counts.most_common(10) . list_of_words = [] list_of_counts = [] for i in w: list_of_words.append(i[0]) list_of_counts.append(i[1]) . title_df = pd.DataFrame({ &#39;text&#39;: list_of_words, &#39;count&#39;: list_of_counts }) . plt.figure(figsize=(20,6)) # Add title plt.title(&quot;Most Frequent Words in the title of the Research Papers Published&quot;) sns.barplot(x=title_df[&#39;text&#39;], y=title_df[&#39;count&#39;]) plt.ylabel(&quot;Count&quot;) . Text(0, 0.5, &#39;Count&#39;) .",
            "url": "https://thedrowsywinger.github.io/Analyzing-COVID-19-Data/2020/02/20/COVID-dataset.html",
            "relUrl": "/2020/02/20/COVID-dataset.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://thedrowsywinger.github.io/Analyzing-COVID-19-Data/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://thedrowsywinger.github.io/Analyzing-COVID-19-Data/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}